{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from datasets import load_multitask_data\n",
    "import bert\n",
    "from config import PretrainedConfig\n",
    "import torch\n",
    "from datasets import SentencePairDataset\n",
    "from tokenizer import BertTokenizer\n",
    "from base_bert import BertPreTrainedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initialization parameters to validate the methods we create\n",
    "class BertConfig(PretrainedConfig):\n",
    "  model_type = \"bert\"\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=0,\n",
    "    gradient_checkpointing=False,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    name_or_path = \"checkpoint\",\n",
    "    **kwargs\n",
    "  ):\n",
    "    super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_hidden_layers = num_hidden_layers\n",
    "    self.num_attention_heads = num_attention_heads\n",
    "    self.hidden_act = hidden_act\n",
    "    self.intermediate_size = intermediate_size\n",
    "    self.hidden_dropout_prob = hidden_dropout_prob\n",
    "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "    self.max_position_embeddings = max_position_embeddings\n",
    "    self.type_vocab_size = type_vocab_size\n",
    "    self.initializer_range = initializer_range\n",
    "    self.layer_norm_eps = layer_norm_eps\n",
    "    self.gradient_checkpointing = gradient_checkpointing\n",
    "    self.position_embedding_type = position_embedding_type\n",
    "    self.use_cache = use_cache\n",
    "    self.name_or_path = name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sst_train SST_TRAIN] [--sst_dev SST_DEV]\n",
      "                             [--sst_test SST_TEST] [--para_train PARA_TRAIN]\n",
      "                             [--para_dev PARA_DEV] [--para_test PARA_TEST]\n",
      "                             [--sts_train STS_TRAIN] [--sts_dev STS_DEV]\n",
      "                             [--sts_test STS_TEST] [--seed SEED]\n",
      "                             [--epochs EPOCHS] [--option {pretrain,finetune}]\n",
      "                             [--use_gpu] [--sst_dev_out SST_DEV_OUT]\n",
      "                             [--sst_test_out SST_TEST_OUT]\n",
      "                             [--para_dev_out PARA_DEV_OUT]\n",
      "                             [--para_test_out PARA_TEST_OUT]\n",
      "                             [--sts_dev_out STS_DEV_OUT]\n",
      "                             [--sts_test_out STS_TEST_OUT]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                             [--lr LR] [--local_files_only]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"fcb29820-1768-4a3f-bb4a-4cf0349d3cd8\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\kacpe\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-9668diCsZ23xRWAf.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kacpe\\anaconda3\\envs\\dnlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import time, random, numpy as np, argparse, sys, re, os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert import BertModel\n",
    "from optimizer import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import SentenceClassificationDataset, SentencePairDataset, \\\n",
    "    load_multitask_data, load_multitask_test_data\n",
    "\n",
    "from evaluation import model_eval_sst, test_model_multitask\n",
    "\n",
    "\n",
    "TQDM_DISABLE=True\n",
    "\n",
    "# fix the random seed\n",
    "def seed_everything(seed=11711):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "BERT_HIDDEN_SIZE = 768\n",
    "N_SENTIMENT_CLASSES = 5\n",
    "\n",
    "\n",
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        #self.num_labels = config.num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')#, local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        ## sst - sentiment, para, sts - semantic text similarity    \n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sst)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_para)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sts)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        out = self.linear(dropped)\n",
    "        sentence_embeddings = out.last_hidden_state[:, 0, :]\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "    def predict_sentiment(self, input_ids, attention_mask):\n",
    "        '''Given a batch of sentences, outputs logits for classifying sentiment.\n",
    "        There are 5 sentiment classes:\n",
    "        (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)\n",
    "        Thus, your output should contain 5 logits for each sentence.\n",
    "        '''\n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict_paraphrase(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict_similarity(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, args, config, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")\n",
    "\n",
    "\n",
    "## Currently only trains on sst dataset\n",
    "def train_multitask(args):\n",
    "    device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "    # Load data\n",
    "    # Create the data and its corresponding datasets and dataloader\n",
    "    sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "    sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train')\n",
    "\n",
    "    sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "    sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "    sst_train_dataloader = DataLoader(sst_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                      collate_fn=sst_train_data.collate_fn)\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_dev_data.collate_fn)\n",
    "\n",
    "    # Init model\n",
    "    config = {'hidden_dropout_prob': args.hidden_dropout_prob,\n",
    "              'num_labels': num_labels,\n",
    "              'hidden_size': 768,\n",
    "              'data_dir': '.',\n",
    "              'option': args.option,\n",
    "              'local_files_only': args.local_files_only}\n",
    "\n",
    "    config = SimpleNamespace(**config)\n",
    "\n",
    "    model = MultitaskBERT(config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lr = args.lr\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    # Run for the specified number of epochs\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in tqdm(sst_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "            b_ids, b_mask, b_labels = (batch['token_ids'],\n",
    "                                       batch['attention_mask'], batch['labels'])\n",
    "\n",
    "            b_ids = b_ids.to(device)\n",
    "            b_mask = b_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.predict_sentiment(b_ids, b_mask)\n",
    "            loss = F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args.batch_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss = train_loss / (num_batches)\n",
    "\n",
    "        train_acc, train_f1, *_ = model_eval_sst(sst_train_dataloader, model, device)\n",
    "        dev_acc, dev_f1, *_ = model_eval_sst(sst_dev_dataloader, model, device)\n",
    "\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            save_model(model, optimizer, args, config, args.filepath)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, train acc :: {train_acc :.3f}, dev acc :: {dev_acc :.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "def test_model(args):\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "        saved = torch.load(args.filepath)\n",
    "        config = saved['model_config']\n",
    "\n",
    "        model = MultitaskBERT(config)\n",
    "        model.load_state_dict(saved['model'])\n",
    "        model = model.to(device)\n",
    "        print(f\"Loaded model to test from {args.filepath}\")\n",
    "\n",
    "        test_model_multitask(args, model, device)\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--sst_train\", type=str, default=\"data/ids-sst-train.csv\")\n",
    "    parser.add_argument(\"--sst_dev\", type=str, default=\"data/ids-sst-dev.csv\")\n",
    "    parser.add_argument(\"--sst_test\", type=str, default=\"data/ids-sst-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--para_train\", type=str, default=\"data/quora-train.csv\")\n",
    "    parser.add_argument(\"--para_dev\", type=str, default=\"data/quora-dev.csv\")\n",
    "    parser.add_argument(\"--para_test\", type=str, default=\"data/quora-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sts_train\", type=str, default=\"data/sts-train.csv\")\n",
    "    parser.add_argument(\"--sts_dev\", type=str, default=\"data/sts-dev.csv\")\n",
    "    parser.add_argument(\"--sts_test\", type=str, default=\"data/sts-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--seed\", type=int, default=11711)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--option\", type=str,\n",
    "                        help='pretrain: the BERT parameters are frozen; finetune: BERT parameters are updated',\n",
    "                        choices=('pretrain', 'finetune'), default=\"pretrain\")\n",
    "    parser.add_argument(\"--use_gpu\", action='store_true')\n",
    "\n",
    "    parser.add_argument(\"--sst_dev_out\", type=str, default=\"predictions/sst-dev-output.csv\")\n",
    "    parser.add_argument(\"--sst_test_out\", type=str, default=\"predictions/sst-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--para_dev_out\", type=str, default=\"predictions/para-dev-output.csv\")\n",
    "    parser.add_argument(\"--para_test_out\", type=str, default=\"predictions/para-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sts_dev_out\", type=str, default=\"predictions/sts-dev-output.csv\")\n",
    "    parser.add_argument(\"--sts_test_out\", type=str, default=\"predictions/sts-test-output.csv\")\n",
    "\n",
    "    # hyper parameters\n",
    "    parser.add_argument(\"--batch_size\", help='sst: 64 can fit a 12GB GPU', type=int, default=64)\n",
    "    parser.add_argument(\"--hidden_dropout_prob\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"learning rate, default lr for 'pretrain': 1e-3, 'finetune': 1e-5\",\n",
    "                        default=1e-3)\n",
    "    parser.add_argument(\"--local_files_only\", action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    args.filepath = f'{args.option}-{args.epochs}-{args.lr}-multitask.pt' # save path\n",
    "    seed_everything(args.seed)  # fix the seed for reproducibility\n",
    "    train_multitask(args)\n",
    "    test_model(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        #self.num_labels = config.num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')#, local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ## sst - sentiment, para, sts - semantic text similarity\n",
    "        self.num_labels_sst = 5    \n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sst)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        out = self.linear(dropped)\n",
    "        sentence_embeddings = out.last_hidden_state[:, 0, :]\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import BertModel\n",
    "config = BertConfig(option='finetune')\n",
    "bert_mod = bert.BertModel(config)\n",
    "\n",
    "model = MultitaskBERT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_type_embedding = nn.Embedding(30522, 768)\n",
    "random_integers = torch.randint(low=0, high=100, size=(100, 100), dtype=torch.int32)\n",
    "\n",
    "\n",
    "\n",
    "    # Get word embedding from self.word_embedding into input_embeds.\n",
    "\n",
    "tk_type_ids = torch.zeros(random_integers.size(), dtype=torch.long)\n",
    "tk_type_embeds = tk_type_embedding(tk_type_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_type_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(BertPreTrainedModel):\n",
    "  \"\"\"\n",
    "  the bert model returns the final embeddings for each token in a sentence\n",
    "  it consists\n",
    "  1. embedding (used in self.embed)\n",
    "  2. a stack of n bert layers (used in self.encode)\n",
    "  3. a linear transformation layer for [CLS] token (used in self.forward, as given)\n",
    "  \"\"\"\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    self.config = config\n",
    "\n",
    "    # embedding\n",
    "    self.word_embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "    self.pos_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "    self.tk_type_embedding = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "    self.embed_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    self.embed_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    # position_ids (1, len position emb) is a constant, register to buffer\n",
    "    position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)\n",
    "    self.register_buffer('position_ids', position_ids)\n",
    "\n",
    "    # bert encoder\n",
    "    self.bert_layers = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    # for [CLS] token\n",
    "    self.pooler_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.pooler_af = nn.Tanh()\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def embed(self, input_ids):\n",
    "    input_shape = input_ids.size()\n",
    "    seq_length = input_shape[1]\n",
    "\n",
    "    # Get word embedding from self.word_embedding into input_embeds.\n",
    "    inputs_embeds = self.word_embedding(input_ids)\n",
    "\n",
    "    # Get position index and position embedding from self.pos_embedding into pos_embeds.\n",
    "    pos_ids = self.position_ids[:, :seq_length] #subsets a list of positions 0:512 to 0:seq_length. \n",
    "\n",
    "    pos_embeds = self.pos_embedding(pos_ids)\n",
    "\n",
    "\n",
    "    # Get token type ids, since we are not consider token type, just a placeholder.\n",
    "    #tk_type_ids = torch.zeros(input_shape, dtype=torch.long, device=input_ids.device)\n",
    "    tk_type_ids = create_attention_mask(input_ids)\n",
    "    tk_type_embeds = self.tk_type_embedding(tk_type_ids)\n",
    "\n",
    "    # Add three embeddings together; then apply embed_layer_norm and dropout and return.\n",
    "    hidden_states = inputs_embeds+pos_embeds+tk_type_embeds\n",
    "    hidden_states = self.embed_layer_norm(hidden_states)\n",
    "    hidden_states = self.embed_dropout(hidden_states)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "  def encode(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    hidden_states: the output from the embedding layer [batch_size, seq_len, hidden_size]\n",
    "    attention_mask: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    # get the extended attention mask for self attention\n",
    "    # returns extended_attention_mask of [batch_size, 1, 1, seq_len]\n",
    "    # non-padding tokens with 0 and padding tokens with a large negative number \n",
    "    extended_attention_mask: torch.Tensor = get_extended_attention_mask(attention_mask, self.dtype)\n",
    "\n",
    "    # pass the hidden states through the encoder layers\n",
    "    for i, layer_module in enumerate(self.bert_layers):\n",
    "      # feed the encoding from the last bert_layer to the next\n",
    "      hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    input_ids: [batch_size, seq_len], seq_len is the max length of the batch\n",
    "    attention_mask: same size as input_ids, 1 represents non-padding tokens, 0 represents padding tokens\n",
    "    \"\"\"\n",
    "    # get the embedding for each input token\n",
    "    embedding_output = self.embed(input_ids=input_ids)\n",
    "\n",
    "    # feed to a transformer (a stack of BertLayers)\n",
    "    sequence_output = self.encode(embedding_output, attention_mask=attention_mask)\n",
    "\n",
    "    # get cls token hidden state\n",
    "    first_tk = sequence_output[:, 0]\n",
    "    first_tk = self.pooler_dense(first_tk)\n",
    "    first_tk = self.pooler_af(first_tk)\n",
    "\n",
    "    return {'last_hidden_state': sequence_output, 'pooler_output': first_tk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig()\n",
    "position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)\n",
    "position_ids.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
