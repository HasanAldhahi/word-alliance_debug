{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from datasets import load_multitask_data\n",
    "import bert\n",
    "from config import PretrainedConfig\n",
    "import torch\n",
    "from datasets import SentencePairDataset\n",
    "from tokenizer import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initialization parameters to validate the methods we create\n",
    "class BertConfig(PretrainedConfig):\n",
    "  model_type = \"bert\"\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=0,\n",
    "    gradient_checkpointing=False,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    name_or_path = \"checkpoint\",\n",
    "    **kwargs\n",
    "  ):\n",
    "    super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_hidden_layers = num_hidden_layers\n",
    "    self.num_attention_heads = num_attention_heads\n",
    "    self.hidden_act = hidden_act\n",
    "    self.intermediate_size = intermediate_size\n",
    "    self.hidden_dropout_prob = hidden_dropout_prob\n",
    "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "    self.max_position_embeddings = max_position_embeddings\n",
    "    self.type_vocab_size = type_vocab_size\n",
    "    self.initializer_range = initializer_range\n",
    "    self.layer_norm_eps = layer_norm_eps\n",
    "    self.gradient_checkpointing = gradient_checkpointing\n",
    "    self.position_embedding_type = position_embedding_type\n",
    "    self.use_cache = use_cache\n",
    "    self.name_or_path = name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sst_train SST_TRAIN] [--sst_dev SST_DEV]\n",
      "                             [--sst_test SST_TEST] [--para_train PARA_TRAIN]\n",
      "                             [--para_dev PARA_DEV] [--para_test PARA_TEST]\n",
      "                             [--sts_train STS_TRAIN] [--sts_dev STS_DEV]\n",
      "                             [--sts_test STS_TEST] [--seed SEED]\n",
      "                             [--epochs EPOCHS] [--option {pretrain,finetune}]\n",
      "                             [--use_gpu] [--sst_dev_out SST_DEV_OUT]\n",
      "                             [--sst_test_out SST_TEST_OUT]\n",
      "                             [--para_dev_out PARA_DEV_OUT]\n",
      "                             [--para_test_out PARA_TEST_OUT]\n",
      "                             [--sts_dev_out STS_DEV_OUT]\n",
      "                             [--sts_test_out STS_TEST_OUT]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                             [--lr LR] [--local_files_only]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"5ee05718-3f9b-462a-b5d7-de0b7a84d08e\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\kacpe\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-106324HO9zVKjTEqr.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kacpe\\anaconda3\\envs\\dnlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import time, random, numpy as np, argparse, sys, re, os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert import BertModel\n",
    "from optimizer import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import SentenceClassificationDataset, SentencePairDataset, \\\n",
    "    load_multitask_data, load_multitask_test_data\n",
    "\n",
    "from evaluation import model_eval_sst, test_model_multitask\n",
    "\n",
    "\n",
    "TQDM_DISABLE=True\n",
    "\n",
    "# fix the random seed\n",
    "def seed_everything(seed=11711):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "BERT_HIDDEN_SIZE = 768\n",
    "N_SENTIMENT_CLASSES = 5\n",
    "\n",
    "\n",
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        #self.num_labels = config.num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')#, local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        ## sst - sentiment, para, sts - semantic text similarity    \n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sst)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_para)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sts)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        out = self.linear(dropped)\n",
    "        sentence_embeddings = out.last_hidden_state[:, 0, :]\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "    def predict_sentiment(self, input_ids, attention_mask):\n",
    "        '''Given a batch of sentences, outputs logits for classifying sentiment.\n",
    "        There are 5 sentiment classes:\n",
    "        (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)\n",
    "        Thus, your output should contain 5 logits for each sentence.\n",
    "        '''\n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict_paraphrase(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict_similarity(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, args, config, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")\n",
    "\n",
    "\n",
    "## Currently only trains on sst dataset\n",
    "def train_multitask(args):\n",
    "    device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "    # Load data\n",
    "    # Create the data and its corresponding datasets and dataloader\n",
    "    sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "    sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train')\n",
    "\n",
    "    sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "    sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "    sst_train_dataloader = DataLoader(sst_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                      collate_fn=sst_train_data.collate_fn)\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_dev_data.collate_fn)\n",
    "\n",
    "    # Init model\n",
    "    config = {'hidden_dropout_prob': args.hidden_dropout_prob,\n",
    "              'num_labels': num_labels,\n",
    "              'hidden_size': 768,\n",
    "              'data_dir': '.',\n",
    "              'option': args.option,\n",
    "              'local_files_only': args.local_files_only}\n",
    "\n",
    "    config = SimpleNamespace(**config)\n",
    "\n",
    "    model = MultitaskBERT(config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lr = args.lr\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    # Run for the specified number of epochs\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in tqdm(sst_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "            b_ids, b_mask, b_labels = (batch['token_ids'],\n",
    "                                       batch['attention_mask'], batch['labels'])\n",
    "\n",
    "            b_ids = b_ids.to(device)\n",
    "            b_mask = b_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.predict_sentiment(b_ids, b_mask)\n",
    "            loss = F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args.batch_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss = train_loss / (num_batches)\n",
    "\n",
    "        train_acc, train_f1, *_ = model_eval_sst(sst_train_dataloader, model, device)\n",
    "        dev_acc, dev_f1, *_ = model_eval_sst(sst_dev_dataloader, model, device)\n",
    "\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            save_model(model, optimizer, args, config, args.filepath)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, train acc :: {train_acc :.3f}, dev acc :: {dev_acc :.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "def test_model(args):\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "        saved = torch.load(args.filepath)\n",
    "        config = saved['model_config']\n",
    "\n",
    "        model = MultitaskBERT(config)\n",
    "        model.load_state_dict(saved['model'])\n",
    "        model = model.to(device)\n",
    "        print(f\"Loaded model to test from {args.filepath}\")\n",
    "\n",
    "        test_model_multitask(args, model, device)\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--sst_train\", type=str, default=\"data/ids-sst-train.csv\")\n",
    "    parser.add_argument(\"--sst_dev\", type=str, default=\"data/ids-sst-dev.csv\")\n",
    "    parser.add_argument(\"--sst_test\", type=str, default=\"data/ids-sst-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--para_train\", type=str, default=\"data/quora-train.csv\")\n",
    "    parser.add_argument(\"--para_dev\", type=str, default=\"data/quora-dev.csv\")\n",
    "    parser.add_argument(\"--para_test\", type=str, default=\"data/quora-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sts_train\", type=str, default=\"data/sts-train.csv\")\n",
    "    parser.add_argument(\"--sts_dev\", type=str, default=\"data/sts-dev.csv\")\n",
    "    parser.add_argument(\"--sts_test\", type=str, default=\"data/sts-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--seed\", type=int, default=11711)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--option\", type=str,\n",
    "                        help='pretrain: the BERT parameters are frozen; finetune: BERT parameters are updated',\n",
    "                        choices=('pretrain', 'finetune'), default=\"pretrain\")\n",
    "    parser.add_argument(\"--use_gpu\", action='store_true')\n",
    "\n",
    "    parser.add_argument(\"--sst_dev_out\", type=str, default=\"predictions/sst-dev-output.csv\")\n",
    "    parser.add_argument(\"--sst_test_out\", type=str, default=\"predictions/sst-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--para_dev_out\", type=str, default=\"predictions/para-dev-output.csv\")\n",
    "    parser.add_argument(\"--para_test_out\", type=str, default=\"predictions/para-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sts_dev_out\", type=str, default=\"predictions/sts-dev-output.csv\")\n",
    "    parser.add_argument(\"--sts_test_out\", type=str, default=\"predictions/sts-test-output.csv\")\n",
    "\n",
    "    # hyper parameters\n",
    "    parser.add_argument(\"--batch_size\", help='sst: 64 can fit a 12GB GPU', type=int, default=64)\n",
    "    parser.add_argument(\"--hidden_dropout_prob\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"learning rate, default lr for 'pretrain': 1e-3, 'finetune': 1e-5\",\n",
    "                        default=1e-3)\n",
    "    parser.add_argument(\"--local_files_only\", action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    args.filepath = f'{args.option}-{args.epochs}-{args.lr}-multitask.pt' # save path\n",
    "    seed_everything(args.seed)  # fix the seed for reproducibility\n",
    "    train_multitask(args)\n",
    "    test_model(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        #self.num_labels = config.num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')#, local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        ## sst - sentiment, para, sts - semantic text similarity    \n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sst)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_para)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sts)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        out = self.linear(dropped)\n",
    "        sentence_embeddings = out.last_hidden_state[:, 0, :]\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertConfig' object has no attribute 'option'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m bert_mod \u001b[39m=\u001b[39m bert\u001b[39m.\u001b[39mBertModel(config)\n\u001b[0;32m      3\u001b[0m config \u001b[39m=\u001b[39m BertConfig()\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m MultitaskBERT(config)\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mMultitaskBERT.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert \u001b[39m=\u001b[39m BertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m#, local_files_only=config.local_files_only)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39;49moption \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpretrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     17\u001b[0m         param\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39moption \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfinetune\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertConfig' object has no attribute 'option'"
     ]
    }
   ],
   "source": [
    "from bert import BertModel\n",
    "bert_mod = bert.BertModel(config)\n",
    "config = BertConfig()\n",
    "model = MultitaskBERT(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
