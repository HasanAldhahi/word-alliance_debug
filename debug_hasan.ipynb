{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from datasets import load_multitask_data\n",
    "import bert\n",
    "from config import PretrainedConfig\n",
    "import torch\n",
    "from datasets import SentencePairDataset\n",
    "from tokenizer import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initialization parameters to validate the methods we create\n",
    "class BertConfig(PretrainedConfig):\n",
    "  model_type = \"bert\"\n",
    "\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    num_labels_sst ,\n",
    "    num_labels_para ,\n",
    "    num_labels_sts ,\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=0,\n",
    "    gradient_checkpointing=False,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    \n",
    "    \n",
    "    \n",
    "    name_or_path = \"checkpoint\",\n",
    "    **kwargs\n",
    "  ):\n",
    "    super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_hidden_layers = num_hidden_layers\n",
    "    self.num_attention_heads = num_attention_heads\n",
    "    self.hidden_act = hidden_act\n",
    "    self.intermediate_size = intermediate_size\n",
    "    self.hidden_dropout_prob = hidden_dropout_prob\n",
    "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "    self.max_position_embeddings = max_position_embeddings\n",
    "    self.type_vocab_size = type_vocab_size\n",
    "    self.initializer_range = initializer_range\n",
    "    self.layer_norm_eps = layer_norm_eps\n",
    "    self.gradient_checkpointing = gradient_checkpointing\n",
    "    self.position_embedding_type = position_embedding_type\n",
    "    self.use_cache = use_cache\n",
    "    self.name_or_path = name_or_path\n",
    "    # added by hasan for multitask\n",
    "    self.num_labels_sst = num_labels_sst\n",
    "    self.num_labels_para = num_labels_para\n",
    "    self.num_labels_sts = num_labels_sts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sst_train SST_TRAIN] [--sst_dev SST_DEV]\n",
      "                             [--sst_test SST_TEST] [--para_train PARA_TRAIN]\n",
      "                             [--para_dev PARA_DEV] [--para_test PARA_TEST]\n",
      "                             [--sts_train STS_TRAIN] [--sts_dev STS_DEV]\n",
      "                             [--sts_test STS_TEST] [--seed SEED]\n",
      "                             [--epochs EPOCHS] [--option {pretrain,finetune}]\n",
      "                             [--use_gpu] [--sst_dev_out SST_DEV_OUT]\n",
      "                             [--sst_test_out SST_TEST_OUT]\n",
      "                             [--para_dev_out PARA_DEV_OUT]\n",
      "                             [--para_test_out PARA_TEST_OUT]\n",
      "                             [--sts_dev_out STS_DEV_OUT]\n",
      "                             [--sts_test_out STS_TEST_OUT]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                             [--lr LR] [--local_files_only]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"3795250a-0e3c-4ec8-93dc-3b1294dd976f\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\Hassan\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-179725n42wzoWp9bI.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\apps\\Anaconda\\envs\\dnlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import time, random, numpy as np, argparse, sys, re, os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert import BertModel\n",
    "from optimizer import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import SentenceClassificationDataset, SentencePairDataset, \\\n",
    "    load_multitask_data, load_multitask_test_data\n",
    "\n",
    "from evaluation import model_eval_sst, test_model_multitask\n",
    "\n",
    "\n",
    "TQDM_DISABLE=True\n",
    "\n",
    "# fix the random seed\n",
    "def seed_everything(seed=11711):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "BERT_HIDDEN_SIZE = 768\n",
    "N_SENTIMENT_CLASSES = 5\n",
    "\n",
    "\n",
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        self.num_labels_sst = config.num_labels_sst\n",
    "        self.num_labels_para = config.num_labels_para\n",
    "        self.num_labels_sts = config.num_labels_sts\n",
    "\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')#, local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        ## sst - sentiment, para, sts - semantic text similarity    \n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sst)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_para)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sts)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        out = self.linear(dropped)\n",
    "        sentence_embeddings = out.last_hidden_state[:, 0, :]\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "    def predict_sentiment(self, input_ids, attention_mask):\n",
    "        '''Given a batch of sentences, outputs logits for classifying sentiment.\n",
    "        There are 5 sentiment classes:\n",
    "        (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)\n",
    "        Thus, your output should contain 5 logits for each sentence.\n",
    "        '''\n",
    "        ### TODO\n",
    "        \n",
    "\n",
    "\n",
    "    def predict_paraphrase(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict_similarity(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, args, config, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")\n",
    "\n",
    "\n",
    "## Currently only trains on sst dataset\n",
    "def train_multitask(args):\n",
    "    device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "    # Load data\n",
    "    # Create the data and its corresponding datasets and dataloader\n",
    "    sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "    sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train')\n",
    "\n",
    "    sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "    sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "    sst_train_dataloader = DataLoader(sst_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                      collate_fn=sst_train_data.collate_fn)\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_dev_data.collate_fn)\n",
    "\n",
    "    # Init model\n",
    "    config = {'hidden_dropout_prob': args.hidden_dropout_prob,\n",
    "              'num_labels': num_labels,\n",
    "              'hidden_size': 768,\n",
    "              'data_dir': '.',\n",
    "              'option': args.option,\n",
    "              'local_files_only': args.local_files_only}\n",
    "\n",
    "    config = SimpleNamespace(**config)\n",
    "\n",
    "    model = MultitaskBERT(config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lr = args.lr\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    # Run for the specified number of epochs\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in tqdm(sst_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "            b_ids, b_mask, b_labels = (batch['token_ids'],\n",
    "                                       batch['attention_mask'], batch['labels'])\n",
    "\n",
    "            b_ids = b_ids.to(device)\n",
    "            b_mask = b_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.predict_sentiment(b_ids, b_mask)\n",
    "            loss = F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args.batch_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss = train_loss / (num_batches)\n",
    "\n",
    "        train_acc, train_f1, *_ = model_eval_sst(sst_train_dataloader, model, device)\n",
    "        dev_acc, dev_f1, *_ = model_eval_sst(sst_dev_dataloader, model, device)\n",
    "\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            save_model(model, optimizer, args, config, args.filepath)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, train acc :: {train_acc :.3f}, dev acc :: {dev_acc :.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "def test_model(args):\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "        saved = torch.load(args.filepath)\n",
    "        config = saved['model_config']\n",
    "\n",
    "        model = MultitaskBERT(config)\n",
    "        model.load_state_dict(saved['model'])\n",
    "        model = model.to(device)\n",
    "        print(f\"Loaded model to test from {args.filepath}\")\n",
    "\n",
    "        test_model_multitask(args, model, device)\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--sst_train\", type=str, default=\"data/ids-sst-train.csv\")\n",
    "    parser.add_argument(\"--sst_dev\", type=str, default=\"data/ids-sst-dev.csv\")\n",
    "    parser.add_argument(\"--sst_test\", type=str, default=\"data/ids-sst-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--para_train\", type=str, default=\"data/quora-train.csv\")\n",
    "    parser.add_argument(\"--para_dev\", type=str, default=\"data/quora-dev.csv\")\n",
    "    parser.add_argument(\"--para_test\", type=str, default=\"data/quora-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sts_train\", type=str, default=\"data/sts-train.csv\")\n",
    "    parser.add_argument(\"--sts_dev\", type=str, default=\"data/sts-dev.csv\")\n",
    "    parser.add_argument(\"--sts_test\", type=str, default=\"data/sts-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--seed\", type=int, default=11711)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2)\n",
    "    parser.add_argument(\"--option\", type=str,\n",
    "                        help='pretrain: the BERT parameters are frozen; finetune: BERT parameters are updated',\n",
    "                        choices=('pretrain', 'finetune'), default=\"pretrain\")\n",
    "    parser.add_argument(\"--use_gpu\", action='store_true')\n",
    "\n",
    "    parser.add_argument(\"--sst_dev_out\", type=str, default=\"predictions/sst-dev-output.csv\")\n",
    "    parser.add_argument(\"--sst_test_out\", type=str, default=\"predictions/sst-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--para_dev_out\", type=str, default=\"predictions/para-dev-output.csv\")\n",
    "    parser.add_argument(\"--para_test_out\", type=str, default=\"predictions/para-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sts_dev_out\", type=str, default=\"predictions/sts-dev-output.csv\")\n",
    "    parser.add_argument(\"--sts_test_out\", type=str, default=\"predictions/sts-test-output.csv\")\n",
    "\n",
    "    # hyper parameters\n",
    "    parser.add_argument(\"--batch_size\", help='sst: 64 can fit a 12GB GPU', type=int, default=64)\n",
    "    parser.add_argument(\"--hidden_dropout_prob\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"learning rate, default lr for 'pretrain': 1e-3, 'finetune': 1e-5\",\n",
    "                        default=1e-3)\n",
    "    parser.add_argument(\"--local_files_only\", action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# fresh start \n",
    "\n",
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        #self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.num_labels_sst = config.num_labels_sst\n",
    "        self.num_labels_para = config.num_labels_para\n",
    "        self.num_labels_sts = config.num_labels_sts\n",
    "\n",
    "\n",
    "        \n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        ## sst - sentiment, para, sts - semantic text similarity    \n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sst)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_para)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels_sts)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        out = self.linear(dropped)\n",
    "        sentence_embeddings = out.last_hidden_state[:, 0, :]\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def predict_sentiment(self, input_ids, attention_mask):\n",
    "        '''Given a batch of sentences, outputs logits for classifying sentiment.\n",
    "        There are 5 sentiment classes:\n",
    "        (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)\n",
    "        Thus, your output should contain 5 logits for each sentence.\n",
    "        '''\n",
    "   \n",
    "       # Get the BERT embeddings\n",
    "        bert_out = self.bert(input_ids, attention_mask)\n",
    "        pooled_output = bert_out['pooler_output']\n",
    "        dropped = self.drop(pooled_output)\n",
    "\n",
    "        # Predict sentiment\n",
    "        sentiment_logits = self.sst_classifier(dropped)\n",
    "\n",
    "        return sentiment_logits\n",
    "    \n",
    "    def predict_paraphrase(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "            # Get the BERT embeddings for both sets of input sentences\n",
    "        bert_out_1 = self.bert(input_ids_1, attention_mask_1)\n",
    "        bert_out_2 = self.bert(input_ids_2, attention_mask_2)\n",
    "        \n",
    "        pooled_output_1 = bert_out_1['pooler_output']\n",
    "        pooled_output_2 = bert_out_2['pooler_output']\n",
    "        \n",
    "        # Combine the pooled embeddings of both sentences using concatenation\n",
    "        combined_pooled = torch.cat((pooled_output_1, pooled_output_2), dim=-1)\n",
    "        dropped = self.drop(combined_pooled)\n",
    "        \n",
    "        # Predict paraphrase\n",
    "        paraphrase_logits = self.para_classifier(dropped)\n",
    "        \n",
    "        return paraphrase_logits\n",
    "     \n",
    "\n",
    "\n",
    "    def predict_similarity(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "         # Get the BERT embeddings for both sets of input sentences\n",
    "        bert_out_1 = self.bert(input_ids_1, attention_mask_1)\n",
    "        bert_out_2 = self.bert(input_ids_2, attention_mask_2)\n",
    "        \n",
    "        pooled_output_1 = bert_out_1['pooler_output']\n",
    "        pooled_output_2 = bert_out_2['pooler_output']\n",
    "        \n",
    "        # Combine the pooled embeddings of both sentences using concatenation\n",
    "        combined_pooled = torch.cat((pooled_output_1, pooled_output_2), dim=-1)\n",
    "        dropped = self.drop(combined_pooled)\n",
    "        \n",
    "        # Predict similarity\n",
    "        similarity_logits = self.sts_classifier(dropped)\n",
    "        \n",
    "        return similarity_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import BertModel\n",
    "# bert_mod = bert.BertModel(config)\n",
    "config = BertConfig(5,1,1)\n",
    "config.option = 'pretrain'\n",
    "model = MultitaskBERT(config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--sst_train\", type=str, default=\"data/ids-sst-train.csv\")\n",
    "    parser.add_argument(\"--sst_dev\", type=str, default=\"data/ids-sst-dev.csv\")\n",
    "    parser.add_argument(\"--sst_test\", type=str, default=\"data/ids-sst-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--para_train\", type=str, default=\"data/quora-train.csv\")\n",
    "    parser.add_argument(\"--para_dev\", type=str, default=\"data/quora-dev.csv\")\n",
    "    parser.add_argument(\"--para_test\", type=str, default=\"data/quora-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sts_train\", type=str, default=\"data/sts-train.csv\")\n",
    "    parser.add_argument(\"--sts_dev\", type=str, default=\"data/sts-dev.csv\")\n",
    "    parser.add_argument(\"--sts_test\", type=str, default=\"data/sts-test-student.csv\")\n",
    "\n",
    "    parser.add_argument(\"--seed\", type=int, default=11711)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--option\", type=str,\n",
    "                        help='pretrain: the BERT parameters are frozen; finetune: BERT parameters are updated',\n",
    "                        choices=('pretrain', 'finetune'), default=\"pretrain\")\n",
    "    parser.add_argument(\"--use_gpu\", action='store_true')\n",
    "\n",
    "    parser.add_argument(\"--sst_dev_out\", type=str, default=\"predictions/sst-dev-output.csv\")\n",
    "    parser.add_argument(\"--sst_test_out\", type=str, default=\"predictions/sst-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--para_dev_out\", type=str, default=\"predictions/para-dev-output.csv\")\n",
    "    parser.add_argument(\"--para_test_out\", type=str, default=\"predictions/para-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sts_dev_out\", type=str, default=\"predictions/sts-dev-output.csv\")\n",
    "    parser.add_argument(\"--sts_test_out\", type=str, default=\"predictions/sts-test-output.csv\")\n",
    "\n",
    "    # hyper parameters\n",
    "    parser.add_argument(\"--batch_size\", help='sst: 64 can fit a 12GB GPU', type=int, default=64)\n",
    "    parser.add_argument(\"--hidden_dropout_prob\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"learning rate, default lr for 'pretrain': 1e-3, 'finetune': 1e-5\",\n",
    "                        default=1e-3)\n",
    "    parser.add_argument(\"--local_files_only\", action='store_true')\n",
    "    \n",
    "    try:\n",
    "        get_ipython().__class__.__name__\n",
    "    # No error means we're running on ipython\n",
    "        args = parser.parse_args(args = []) # Reset args\n",
    "    except NameError:\n",
    "    # NameError means that we're running on terminal\n",
    "        print('Running on terminal')\n",
    "        args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from multitask_classifier import get_args, MultitaskBERT\n",
    "args = get_args()\n",
    "# print(args)\n",
    "args.filepath = f'{args.option}-{args.epochs}-{args.lr}-multitask.pt' # save path\n",
    "seed_everything(args.seed)  # fix the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n",
      "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
      "Loaded 20212 train examples from data/quora-dev.csv\n",
      "Loaded 863 train examples from data/sts-dev.csv\n"
     ]
    }
   ],
   "source": [
    "# load the data set \n",
    "\n",
    "\n",
    " \n",
    "device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "# Load data\n",
    "# Create the data and its corresponding datasets and dataloader\n",
    "sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train')\n",
    "\n",
    "#Sentiment analysis\n",
    "sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "sst_train_dataloader = DataLoader(sst_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_train_data.collate_fn)\n",
    "sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=sst_dev_data.collate_fn)\n",
    "\n",
    "#Paraphrasing\n",
    "paraphrase_train_data = SentencePairDataset(para_train_data, args, isRegression =False)\n",
    "paraphrase_dev_data = SentencePairDataset(para_dev_data, args, isRegression =False)\n",
    "\n",
    "paraphrase_train_dataloader = DataLoader(paraphrase_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                collate_fn=paraphrase_train_data.collate_fn)\n",
    "paraphrase_dev_dataloader = DataLoader(paraphrase_dev_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                collate_fn=paraphrase_dev_data.collate_fn)\n",
    "\n",
    "#sts\n",
    "sts_train_data = SentencePairDataset(sts_train_data, args, isRegression =True)\n",
    "sts_dev_data = SentencePairDataset(sts_dev_data, args, isRegression =True)\n",
    "\n",
    "sts_train_dataloader = DataLoader(sts_train_data, shuffle=True, batch_size=256,\n",
    "                                collate_fn=sts_train_data.collate_fn)\n",
    "sts_dev_dataloader = DataLoader(sts_dev_data, shuffle=True, batch_size=256,\n",
    "                            collate_fn=sts_dev_data.collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model parameters \n",
    "# config = {'hidden_dropout_prob': args.hidden_dropout_prob,\n",
    "#             'num_labels': num_labels,\n",
    "#             'hidden_size': 768,\n",
    "#             'data_dir': '.',\n",
    "#             'option': args.option,\n",
    "#             'local_files_only': args.local_files_only}\n",
    "\n",
    "# config = SimpleNamespace(**config)\n",
    "config = BertConfig(5,1,1)\n",
    "config.option = 'pretrain'\n",
    "\n",
    "model = MultitaskBERT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "best_dev_acc = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m b_labels \u001b[38;5;241m=\u001b[39m b_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, b_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[49], line 56\u001b[0m, in \u001b[0;36mMultitaskBERT.predict_sentiment\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m \u001b[39m\u001b[38;5;124;03m'''Given a batch of sentences, outputs logits for classifying sentiment.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m There are 5 sentiment classes:\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m Thus, your output should contain 5 logits for each sentence.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m '''\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Get the BERT embeddings\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m  bert_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m  pooled_output \u001b[38;5;241m=\u001b[39m bert_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooler_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     58\u001b[0m  dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(pooled_output)\n",
      "File \u001b[1;32md:\\apps\\Anaconda\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Hassan\\Desktop\\NLP\\word-alliance\\bert.py:236\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m    233\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(input_ids\u001b[39m=\u001b[39minput_ids)\n\u001b[0;32m    235\u001b[0m \u001b[39m# feed to a transformer (a stack of BertLayers)\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(embedding_output, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m    238\u001b[0m \u001b[39m# get cls token hidden state\u001b[39;00m\n\u001b[0;32m    239\u001b[0m first_tk \u001b[39m=\u001b[39m sequence_output[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Hassan\\Desktop\\NLP\\word-alliance\\bert.py:223\u001b[0m, in \u001b[0;36mBertModel.encode\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39m# pass the hidden states through the encoder layers\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39mfor\u001b[39;00m i, layer_module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_layers):\n\u001b[0;32m    222\u001b[0m   \u001b[39m# feed the encoding from the last bert_layer to the next\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m   hidden_states \u001b[39m=\u001b[39m layer_module(hidden_states, extended_attention_mask)\n\u001b[0;32m    225\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32md:\\apps\\Anaconda\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Hassan\\Desktop\\NLP\\word-alliance\\bert.py:137\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39mhidden_states: either from the embedding layer (first bert layer) or from the previous bert layer\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39mas shown in the left of Figure 1 of https://arxiv.org/pdf/1706.03762.pdf \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39m4. a add-norm that takes the input and output of the feed forward layer\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m### TODO\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m self_attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention\u001b[39m.\u001b[39;49mforward(hidden_states,attention_mask)\n\u001b[0;32m    139\u001b[0m normalized_attention_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_norm(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mhidden_states, output\u001b[39m=\u001b[39mself_attention_output , \n\u001b[0;32m    140\u001b[0m               dense_layer\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_dense, dropout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_dropout, ln_layer\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_layer_norm)\n\u001b[0;32m    142\u001b[0m ffn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterm_dense(normalized_attention_layer)\n",
      "File \u001b[1;32md:\\Hassan\\Desktop\\NLP\\word-alliance\\bert.py:83\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m     81\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery)\n\u001b[0;32m     82\u001b[0m \u001b[39m# calculate the multi-head attention \u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m attn_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(key_layer, query_layer, value_layer, attention_mask)\n\u001b[0;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m attn_value\n",
      "File \u001b[1;32md:\\Hassan\\Desktop\\NLP\\word-alliance\\bert.py:59\u001b[0m, in \u001b[0;36mBertSelfAttention.attention\u001b[1;34m(self, key, query, value, attention_mask)\u001b[0m\n\u001b[0;32m     57\u001b[0m result_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(result_tensor,dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#Apply softmax\u001b[39;00m\n\u001b[0;32m     58\u001b[0m result_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(result_tensor)\n\u001b[1;32m---> 59\u001b[0m result_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(result_tensor,value)\n\u001b[0;32m     61\u001b[0m result_tensor \u001b[39m=\u001b[39m result_tensor\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[0;32m     63\u001b[0m bs, seq_len \u001b[39m=\u001b[39m result_tensor\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model with the dataset\n",
    "# Run for the specified number of epochs\n",
    "for epoch in range(args.epochs ):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "    for batch in tqdm(sst_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "        b_ids, b_mask, b_labels = (batch['token_ids'],\n",
    "                                    batch['attention_mask'], batch['labels'])\n",
    "\n",
    "        b_ids = b_ids.to(device)\n",
    "        b_mask = b_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model.predict_sentiment(b_ids, b_mask)\n",
    "        loss = F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args.batch_size\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    train_loss = train_loss / (num_batches)\n",
    "\n",
    "    train_acc, train_f1, *_ = model_eval_sst(sst_train_dataloader, model, device)\n",
    "    dev_acc, dev_f1, *_ = model_eval_sst(sst_dev_dataloader, model, device)\n",
    "\n",
    "\n",
    "    ## Adding multitask evaluation\n",
    "    #train\n",
    "    (train_paraphrase_accuracy, train_para_y_pred, train_para_sent_ids,\n",
    "      train_sentiment_accuracy,train_sst_y_pred, train_sst_sent_ids,\n",
    "        train_sts_corr, train_sts_y_pred, train_sts_sent_ids) = model_eval_multitask(sst_train_dataloader,\n",
    "                                                                  paraphrase_train_dataloader,sts_train_dataloader,model, model.device  )\n",
    "    \n",
    "    #dev\n",
    "    (dev_paraphrase_accuracy, dev_para_y_pred, dev_para_sent_ids,\n",
    "      dev_sentiment_accuracy,dev_sst_y_pred, dev_sst_sent_ids,\n",
    "        dev_sts_corr, dev_sts_y_pred, dev__sent_ids) = model_eval_multitask(sst_dev_dataloader,\n",
    "                                                                  paraphrase_dev_dataloader,sts_dev_dataloader,model, model.device  )\n",
    "    \n",
    "    #We have to weight or average the three sores to save the best model.\n",
    "    # In the diven code only sst is used\n",
    "    if dev_acc > best_dev_acc: \n",
    "      best_dev_acc = dev_acc\n",
    "      save_model(model, optimizer, args, config, args.filepath)\n",
    "\n",
    "    print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, train acc :: {train_acc :.3f}, dev acc :: {dev_acc :.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with evaluation dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('dnlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd7499b50a9359b0244c088e31359451aafcf905b1d16c7709f0fd43ea3ea1a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
