{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a56b268-810d-49ab-87d5-c5090cf7cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_multitask_data\n",
    "import bert\n",
    "from config import PretrainedConfig\n",
    "import torch\n",
    "from datasets import SentencePairDataset\n",
    "from tokenizer import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba511a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This library gives a better idea of how to compute the hidden states\n",
    "# https://github.com/codertimo\n",
    "# https://pypi.org/project/bert-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb00015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_pytorch.model.embedding.bert as bert_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f30c7b-5bae-473c-adc1-d1d7b9b99c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\",\n",
       " 3,\n",
       " '32a4f146782cbde1b7fa65799')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load project's datasets\n",
    "sentiment_data, num_labels, paraphrased_data, similarity_data = load_multitask_data(\"data/ids-sst-train.csv\", \"data/quora-train.csv\", \n",
    "                    \"data/sts-train.csv\")\n",
    "#Show example \n",
    "sentiment_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0efc78-e290-42cc-855f-f25a76ee90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initialization parameters to validate the methods we create\n",
    "class BertConfig(PretrainedConfig):\n",
    "  model_type = \"bert\"\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=0,\n",
    "    gradient_checkpointing=False,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    name_or_path = \"checkpoint\",\n",
    "    **kwargs\n",
    "  ):\n",
    "    super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_hidden_layers = num_hidden_layers\n",
    "    self.num_attention_heads = num_attention_heads\n",
    "    self.hidden_act = hidden_act\n",
    "    self.intermediate_size = intermediate_size\n",
    "    self.hidden_dropout_prob = hidden_dropout_prob\n",
    "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "    self.max_position_embeddings = max_position_embeddings\n",
    "    self.type_vocab_size = type_vocab_size\n",
    "    self.initializer_range = initializer_range\n",
    "    self.layer_norm_eps = layer_norm_eps\n",
    "    self.gradient_checkpointing = gradient_checkpointing\n",
    "    self.position_embedding_type = position_embedding_type\n",
    "    self.use_cache = use_cache\n",
    "    self.name_or_path = name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59d73a-4959-42c0-a543-098055647857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0edd4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a61dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding1 = tokenizer(sentence1, return_tensors='pt', padding='max_length', truncation=True,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d34c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding1['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2484260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids_1 torch.Size([1024, 49])\n",
      "token_type_ids_1 torch.Size([1024, 49])\n",
      "attention_mask_1 torch.Size([1024, 49])\n",
      "token_ids_2 torch.Size([1024, 76])\n",
      "token_type_ids_2 torch.Size([1024, 76])\n",
      "attention_mask_2 torch.Size([1024, 76])\n",
      "labels torch.Size([1024])\n",
      "sent_ids 8\n"
     ]
    }
   ],
   "source": [
    "#TODO explore how the sentencePairDataset tokenizes and enumerates the raw data\n",
    "data = SentencePairDataset(paraphrased_data,args=[])\n",
    "BS = 1024 #batch size\n",
    "padded_data = data.collate_fn(paraphrased_data[0:BS])\n",
    "padded_data.keys()\n",
    "for k,v in padded_data.items():\n",
    "    if isinstance(v,list):\n",
    "        print(k,len(k))\n",
    "    else:\n",
    "        print(k,v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aadebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = BertConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5616498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the bert embedding implementation found on github\n",
    "encoder = bert_module.BERTEmbedding(vocab_size=config.vocab_size,embed_size=config.hidden_size)\n",
    "token_ids_encoding = padded_data['token_ids_1']\n",
    "attention_mask = padded_data['attention_mask_1']\n",
    "\n",
    "mistery_number = token_ids_encoding.size()[1]\n",
    "#The mistery number appears to be seq_len which reflects the size of the largest tokenized and encoded input sequence\n",
    "# That is why when we increase the batch size, it is going to change depending on the new largest\n",
    "# tokenized input sequence from that batch.\n",
    "# segment label corresponds to token_types_ids\n",
    "segment_label = padded_data['token_type_ids_1']\n",
    "# We randomized the segment label, but it depends on the one we picked\n",
    "hidden_states = encoder.forward(token_ids_encoding,segment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1f0b6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(importlib.import_module(\"bert\"))\n",
    "bert_mod = bert.BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6741d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attention_score= bert_mod.bert_layers[0].self_attention.forward(hidden_states = hidden_states,attention_mask = attention_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cfdfd4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0480, -0.4725, -0.2700,  ..., -0.2260,  0.4418,  0.5546],\n",
       "        [-0.1600, -0.1787, -0.2360,  ..., -0.2016,  0.4810,  0.3370],\n",
       "        [-0.1533, -0.2538, -0.1650,  ..., -0.2523,  0.5277,  0.4042],\n",
       "        ...,\n",
       "        [-0.1536, -0.3588, -0.2024,  ..., -0.2161,  0.4689,  0.4363],\n",
       "        [-0.1690, -0.3611, -0.1920,  ..., -0.2347,  0.4824,  0.4134],\n",
       "        [-0.1744, -0.3769, -0.1845,  ..., -0.2358,  0.4604,  0.3960]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
