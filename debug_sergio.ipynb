{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is better to copy the code here instead of importing to prevent the arg part from running\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import time, random, numpy as np, argparse\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from types import SimpleNamespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "from datasets import SentenceClassificationDataset, SentencePairDataset, \\\n",
    "    load_multitask_data\n",
    "from bert import BertModel\n",
    "from data_loader import MultiTaskBatchSampler,MultiTaskDataset\n",
    "from optimizer import AdamW\n",
    "\n",
    "from evaluation import model_eval_sst, test_model_multitask, model_eval_multitask, compute_loss_weights\n",
    "from tokenizer import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENTIMENT_CLASSES = 5\n",
    "TQDM_DISABLE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, N_SENTIMENT_CLASSES)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', local_files_only=config.local_files_only)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,token_type_ids):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask,token_type_ids) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        return dropped\n",
    "\n",
    "    def predict(self,input_ids,attention_mask,token_type_ids,task_id):\n",
    "        cls_hidden_state = self.forward(input_ids, attention_mask,token_type_ids)\n",
    "\n",
    "        if task_id==0:\n",
    "            return self.sst_classifier(cls_hidden_state)\n",
    "        elif task_id==1:\n",
    "            return self.para_classifier(cls_hidden_state)\n",
    "        elif task_id==2:\n",
    "            return self.sts_classifier(cls_hidden_state)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task_id value. Expected 0, 1, or 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, args, config, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")\n",
    "\n",
    "#Collate function dependent on current task\n",
    "class CustomCollateFn:\n",
    "    def __init__(self, collate_fns):\n",
    "        self.collate_fns = collate_fns\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        task_id,_= batch[0] #This tuple is defined in the MultiTaskDataset class\n",
    "        #This only works if a batch only contains data from one task\n",
    "        collate_fn = self.collate_fns[task_id]\n",
    "        actual_batch = [actual_batch for _, actual_batch in batch]\n",
    "        return collate_fn(actual_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask(args):\n",
    "    device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "    # Load data\n",
    "    # Create the data and its corresponding datasets and dataloader\n",
    "    sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "    sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train') #Itis correct to use this slit for dev. The other option is test which does not load the labels\n",
    "    \n",
    "    #Sentiment analysis\n",
    "    sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "    sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_dev_data.collate_fn)\n",
    "    \n",
    "    #Paraphrasing\n",
    "    paraphrase_train_data = SentencePairDataset(para_train_data, args, isRegression =False)\n",
    "    paraphrase_dev_data = SentencePairDataset(para_dev_data, args, isRegression =False)\n",
    "\n",
    "    paraphrase_dev_dataloader = DataLoader(paraphrase_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=paraphrase_dev_data.collate_fn)\n",
    "    \n",
    "    #sts\n",
    "    sts_train_data = SentencePairDataset(sts_train_data, args, isRegression =True)\n",
    "    sts_dev_data = SentencePairDataset(sts_dev_data, args, isRegression =True)\n",
    "\n",
    "    sts_dev_dataloader = DataLoader(sts_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=sts_dev_data.collate_fn)\n",
    "    \n",
    "    #MTL data loader\n",
    "    train_datasets = [sst_train_data,paraphrase_train_data, sts_train_data]\n",
    "    #Temporarily initialized here but later in epoch loop to update current epoch and do annealed sampling\n",
    "    mtl_sampler = MultiTaskBatchSampler(        datasets=train_datasets,\n",
    "        current_epoch=1,\n",
    "        total_epochs=args.epochs,\n",
    "        batch_size = args.batch_size,\n",
    "        mix_opt=1,\n",
    "        extra_task_ratio=0,\n",
    "        bin_size=64,\n",
    "        bin_on=False,\n",
    "        bin_grow_ratio=0.5,\n",
    "        sampling='sequential')\n",
    "\n",
    "    multi_task_train_dataset = MultiTaskDataset(train_datasets)\n",
    "\n",
    "    collate_fns = {\n",
    "        0: sst_train_data.collate_fn,\n",
    "        1: paraphrase_train_data.collate_fn,\n",
    "        2: sts_train_data.collate_fn\n",
    "    }\n",
    "\n",
    "    # Creating the custom collate function using the dictionary of collate functions\n",
    "    # Linked to each task id\n",
    "    custom_collate_fn = CustomCollateFn(collate_fns)\n",
    "\n",
    "    multi_task_train_data = DataLoader(\n",
    "    multi_task_train_dataset,\n",
    "    batch_sampler=mtl_sampler,\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "    # Init model\n",
    "    config = {'hidden_dropout_prob': args.hidden_dropout_prob,\n",
    "              'num_labels': num_labels,\n",
    "              'hidden_size': 768,\n",
    "              'data_dir': '.',\n",
    "              'option': args.option,\n",
    "              'local_files_only': args.local_files_only}\n",
    "\n",
    "    config = SimpleNamespace(**config)\n",
    "\n",
    "    model = MultitaskBERT(config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lr = args.lr\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    best_dev_acc_sst = 0\n",
    "    best_dev_acc_paraphrase = 0\n",
    "    best_dev_corr_sts = 0\n",
    "\n",
    "    # Run for the specified number of epochs\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        sst_train_loss_list = []\n",
    "        paraphrase_train_loss_list = []\n",
    "        sts_train_loss_list = []\n",
    "        \n",
    "        for batch in tqdm(multi_task_train_data, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "\n",
    "            #Batch loading, prediction and loss depending on task:\n",
    "            optimizer.zero_grad()\n",
    "            b_task_id, b_ids, b_mask,b_token_type_ids, b_labels = (batch['task_id'],batch['token_ids'],\n",
    "                        batch['attention_mask'],batch['token_type_ids'], batch['labels'])\n",
    "\n",
    "            logits = model.predict(input_ids=b_ids,attention_mask=b_mask,token_type_ids=b_token_type_ids,task_id=b_task_id)\n",
    "            batch_loss = [0]*3\n",
    "            if b_task_id==0: #Sentiment analysis\n",
    "                sst_loss = F.cross_entropy(logits, b_labels.view(-1), reduction='mean')\n",
    "                batch_loss[b_task_id]=sst_loss\n",
    "                sst_train_loss_list.append(sst_loss.item()) #value, not tensor\n",
    "\n",
    "            elif b_task_id==1: #Paraphrasing\n",
    "                bce_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "                paraphrase_loss=bce_loss(logits.view(-1),b_labels.to(torch.float64)) #Change these logits\n",
    "                batch_loss[b_task_id]=paraphrase_loss\n",
    "                paraphrase_train_loss_list.append(paraphrase_loss.item())\n",
    "\n",
    "            elif b_task_id==2: # Text similarity\n",
    "\n",
    "                sigmoid = nn.Sigmoid()\n",
    "                probabilities = sigmoid(logits) #maps logits to range 0 to 1\n",
    "                # Define the MSE loss function\n",
    "                mse_loss = nn.MSELoss(reduction='mean')\n",
    "                b_labels_scaled = (b_labels / 5).float() #Divide between 5 to match range 0 to 1 of logit. Float required due to loss calculation error\n",
    "                sts_loss = mse_loss(probabilities.view(-1), b_labels_scaled)\n",
    "                batch_loss[b_task_id]=sts_loss\n",
    "                sts_train_loss_list.append(sts_loss.item())\n",
    "                        \n",
    "            else:\n",
    "                raise ValueError(\"Invalid b_task_id value. Expected 0, 1, or 2.\")\n",
    "            \n",
    "\n",
    "            losses_list = [sst_train_loss_list,paraphrase_train_loss_list,sts_train_loss_list]\n",
    "            #Compute weighted loss\n",
    "            weights = compute_loss_weights(losses_list)\n",
    "\n",
    "            total_loss = 0\n",
    "            for loss, weight in zip(batch_loss,weights):\n",
    "                total_loss+=loss*weight\n",
    "                \n",
    "            print(\"task number\",b_task_id)\n",
    "            print(\"batch_number\",num_batches)\n",
    "            print(len(b_ids))\n",
    "            if total_loss.dtype == torch.float:\n",
    "                print(\"Tensor has Float data type\")\n",
    "            elif total_loss.dtype == torch.double:\n",
    "                print(\"Tensor has Double data type\")\n",
    "            else:\n",
    "                print(\"Tensor has a different data type\")\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            #End of training batches\n",
    "\n",
    "        #Start dev evaluation \n",
    "        (dev_paraphrase_accuracy, dev_para_y_pred, dev_para_sent_ids,\n",
    "         dev_sentiment_accuracy,dev_sst_y_pred, dev_sst_sent_ids,\n",
    "           dev_sts_corr, dev_sts_y_pred, dev__sent_ids) = model_eval_multitask(sst_dev_dataloader,\n",
    "                                                                      paraphrase_dev_dataloader,sts_dev_dataloader,model, model.device  )\n",
    "        \n",
    "        #We have to weight or average the three sores to save the best model.\n",
    "        # In the diven code only sst is used\n",
    "\n",
    "\n",
    "        if dev_sentiment_accuracy > best_dev_acc_sst and dev_paraphrase_accuracy >best_dev_acc_paraphrase and dev_sts_corr>best_dev_corr_sts:\n",
    "            best_dev_acc_sst = dev_sentiment_accuracy\n",
    "            best_dev_acc_paraphrase = dev_paraphrase_accuracy\n",
    "            best_dev_corr_sts = dev_sts_corr\n",
    "            save_model(model, optimizer, args, config, args.filepath)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, dev sentiment acc :: {dev_paraphrase_accuracy :.3f}, dev sentiment acc :: {dev_paraphrase_accuracy :.3f}, dev sts corr :: {best_dev_corr_sts :.3f}\")\n",
    "        #Add train metrics to this print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tempArgs():\n",
    "    def __init__(self,):\n",
    "        self.local_files_only=True\n",
    "        self.use_gpu = False\n",
    "        self.sst_train=\"data/ids-sst-train.csv\"\n",
    "        self.sst_dev=\"data/ids-sst-dev.csv\"\n",
    "        self.sst_test=\"data/ids-sst-test-student.csv\"\n",
    "        self.para_train=\"data/quora-train.csv\"\n",
    "        self.para_dev=\"data/quora-dev.csv\"\n",
    "        self.para_test=\"data/quora-test-student.csv\"\n",
    "        self.sts_train=\"data/sts-train.csv\"\n",
    "        self.sts_dev=\"data/sts-dev.csv\"\n",
    "        self.sts_test=\"data/sts-test-student.csv\"\n",
    "        self.batch_size = 32\n",
    "        self.hidden_dropout_prob = 0.3\n",
    "        self.option=\"pretrain\"\n",
    "        self.lr = 1e-3\n",
    "        self.epochs = 10\n",
    "tempargs = tempArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n",
      "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
      "Loaded 20212 train examples from data/quora-dev.csv\n",
      "Loaded 863 train examples from data/sts-dev.csv\n",
      "task number 2\n",
      "batch_number 0\n",
      "32\n",
      "Tensor has Float data type\n",
      "task number 2\n",
      "batch_number 1\n",
      "32\n",
      "Tensor has Float data type\n",
      "task number 2\n",
      "batch_number 2\n",
      "32\n",
      "Tensor has Float data type\n",
      "task number 2\n",
      "batch_number 3\n",
      "32\n",
      "Tensor has Float data type\n",
      "task number 2\n",
      "batch_number 4\n",
      "32\n",
      "Tensor has Float data type\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_multitask(tempargs)\n",
      "Cell \u001b[1;32mIn[32], line 96\u001b[0m, in \u001b[0;36mtrain_multitask\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     92\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     93\u001b[0m b_task_id, b_ids, b_mask,b_token_type_ids, b_labels \u001b[39m=\u001b[39m (batch[\u001b[39m'\u001b[39m\u001b[39mtask_id\u001b[39m\u001b[39m'\u001b[39m],batch[\u001b[39m'\u001b[39m\u001b[39mtoken_ids\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     94\u001b[0m             batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m],batch[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 96\u001b[0m logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(input_ids\u001b[39m=\u001b[39;49mb_ids,attention_mask\u001b[39m=\u001b[39;49mb_mask,token_type_ids\u001b[39m=\u001b[39;49mb_token_type_ids,task_id\u001b[39m=\u001b[39;49mb_task_id)\n\u001b[0;32m     97\u001b[0m batch_loss \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m b_task_id\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m: \u001b[39m#Sentiment analysis\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mMultitaskBERT.predict\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, task_id)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m,input_ids,attention_mask,token_type_ids,task_id):\n\u001b[1;32m---> 38\u001b[0m     cls_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_ids, attention_mask,token_type_ids)\n\u001b[0;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m task_id\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msst_classifier(cls_hidden_state)\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mMultitaskBERT.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m'\u001b[39m\u001b[39mTakes a batch of sentences and produces embeddings for them.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[39m# The final BERT embedding is the hidden state of [CLS] token (the first token)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# Here, you can start by just returning the embeddings straight from BERT.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# When thinking of improvements, you can later try modifying this\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m# (e.g., by adding other layers).\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m bert_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(input_ids, attention_mask,token_type_ids) \n\u001b[0;32m     34\u001b[0m dropped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(bert_out[\u001b[39m'\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m dropped\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\sago\\Documents\\GitHub\\word-alliance\\bert.py:233\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m    230\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(input_ids\u001b[39m=\u001b[39minput_ids,token_type_ids\u001b[39m=\u001b[39mtoken_type_ids)\n\u001b[0;32m    232\u001b[0m \u001b[39m# feed to a transformer (a stack of BertLayers)\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(embedding_output, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m    235\u001b[0m \u001b[39m# get cls token hidden state\u001b[39;00m\n\u001b[0;32m    236\u001b[0m first_tk \u001b[39m=\u001b[39m sequence_output[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sago\\Documents\\GitHub\\word-alliance\\bert.py:220\u001b[0m, in \u001b[0;36mBertModel.encode\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m# pass the hidden states through the encoder layers\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39mfor\u001b[39;00m i, layer_module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_layers):\n\u001b[0;32m    219\u001b[0m   \u001b[39m# feed the encoding from the last bert_layer to the next\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m   hidden_states \u001b[39m=\u001b[39m layer_module(hidden_states, extended_attention_mask)\n\u001b[0;32m    222\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\sago\\Documents\\GitHub\\word-alliance\\bert.py:145\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    142\u001b[0m ffn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterm_dense(normalized_attention_layer)\n\u001b[0;32m    143\u001b[0m ffn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterm_af(ffn) \u001b[39m# Activation. After this we get 4th dimension of size 3072\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m normalized_output_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_norm(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mnormalized_attention_layer, output\u001b[39m=\u001b[39;49mffn, \n\u001b[0;32m    146\u001b[0m               dense_layer\u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_dense, dropout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_dropout, ln_layer\u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_layer_norm)\n\u001b[0;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m normalized_output_layer\n",
      "File \u001b[1;32mc:\\Users\\sago\\Documents\\GitHub\\word-alliance\\bert.py:118\u001b[0m, in \u001b[0;36mBertLayer.add_norm\u001b[1;34m(self, input, output, dense_layer, dropout, ln_layer)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39mthis function is applied after the multi-head attention layer or the feed forward layer\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39minput: the input of the previous layer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mln_layer: the layer norm to be applied\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39m# Hint: Remember that BERT applies to the output of each sub-layer, before it is added to the sub-layer input and normalized \u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m \u001b[39m# Each sub-layer in each encoder has a residual connection around it leading to layer-normalisation\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[39m# Need to combine input and output\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m dense_output \u001b[39m=\u001b[39m dense_layer(output)\n\u001b[0;32m    119\u001b[0m \u001b[39m# Apply normalisation\u001b[39;00m\n\u001b[0;32m    120\u001b[0m norm_output \u001b[39m=\u001b[39m dropout(dense_output)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\modules\\linear.py:94\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\functional.py:1753\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[0;32m   1752\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> 1753\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_multitask(tempargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
