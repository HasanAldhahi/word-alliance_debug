{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is better to copy the code here instead of importing to prevent the arg part from running\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import time, random, numpy as np, argparse\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from types import SimpleNamespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "from datasets import SentenceClassificationDataset, SentencePairDataset, \\\n",
    "    load_multitask_data\n",
    "from bert import BertModel\n",
    "from data_loader import MultiTaskBatchSampler,MultiTaskDataset\n",
    "from optimizer import AdamW\n",
    "\n",
    "from evaluation import model_eval_sst, test_model_multitask, model_eval_multitask, compute_loss_weights\n",
    "from tokenizer import BertTokenizer\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tempArgs():\n",
    "    def __init__(self,):\n",
    "        self.local_files_only=True\n",
    "        self.use_gpu = True\n",
    "        self.sst_train=\"data/ids-sst-train.csv\"\n",
    "        self.sst_dev=\"data/ids-sst-dev.csv\"\n",
    "        self.sst_test=\"data/ids-sst-test-student.csv\"\n",
    "        self.para_train=\"data/quora-train.csv\"\n",
    "        self.para_dev=\"data/quora-dev.csv\"\n",
    "        self.para_test=\"data/quora-test-student.csv\"\n",
    "        self.sts_train=\"data/sts-train.csv\"\n",
    "        self.sts_dev=\"data/sts-dev.csv\"\n",
    "        self.sts_test=\"data/sts-test-student.csv\"\n",
    "        self.batch_size = 32\n",
    "        self.hidden_dropout_prob = 0.3\n",
    "        self.option=\"pretrain\"\n",
    "        self.lr = 1e-3\n",
    "        self.epochs = 10\n",
    "        self.filepath='./models/'\n",
    "args = tempArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENTIMENT_CLASSES = 5\n",
    "TQDM_DISABLE=False\n",
    "device = torch.device('cuda') if args.use_gpu else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.device('cuda'))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, N_SENTIMENT_CLASSES)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', local_files_only=config.local_files_only)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,token_type_ids):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask,token_type_ids) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        return dropped\n",
    "\n",
    "    def predict(self,input_ids,attention_mask,token_type_ids,task_id):\n",
    "        cls_hidden_state = self.forward(input_ids, attention_mask,token_type_ids)\n",
    "\n",
    "        if task_id==0:\n",
    "            return self.sst_classifier(cls_hidden_state)\n",
    "        elif task_id==1:\n",
    "            return self.para_classifier(cls_hidden_state)\n",
    "        elif task_id==2:\n",
    "            return self.sts_classifier(cls_hidden_state)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task_id value. Expected 0, 1, or 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "# x = torch.tensor([0, 1, 2, 3, 4])\n",
    "# torch.save(x, 'models/mlt-classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, base_path):\n",
    "    \"\"\"\n",
    "    Save a PyTorch model with a changing number in the filename.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model to be saved.\n",
    "        base_path (str): The base path where the model will be saved.\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    # Find the next available model number\n",
    "   \n",
    "\n",
    "    # Save the model\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def save_model(model, optimizer, args, config, filepath):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "    model_number = 1\n",
    "    while os.path.exists(os.path.join(filepath, f\"model_{model_number}.pt\")):\n",
    "        model_number += 1\n",
    "    model_path = os.path.join(filepath, f\"model_{model_number}.pt\")\n",
    "    torch.save(save_info, model_path)\n",
    "    print(f\"save the model to {filepath}\")\n",
    "\n",
    "#Collate function dependent on current task\n",
    "class CustomCollateFn:\n",
    "    def __init__(self, collate_fns):\n",
    "        self.collate_fns = collate_fns\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        task_id,_= batch[0] #This tuple is defined in the MultiTaskDataset class\n",
    "        #This only works if a batch only contains data from one task\n",
    "        collate_fn = self.collate_fns[task_id]\n",
    "        actual_batch = [actual_batch for _, actual_batch in batch]\n",
    "        return collate_fn(actual_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n",
      "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
      "Loaded 20212 train examples from data/quora-dev.csv\n",
      "Loaded 863 train examples from data/sts-dev.csv\n"
     ]
    }
   ],
   "source": [
    "# def train_multitask(args):\n",
    "\n",
    "# Load data\n",
    "# Create the data and its corresponding datasets and dataloader\n",
    "sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train') #Itis correct to use this slit for dev. The other option is test which does not load the labels\n",
    "\n",
    "#Sentiment analysis\n",
    "sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_dev_data.collate_fn, pin_memory=True )\n",
    "\n",
    "    #Paraphrasing\n",
    "    paraphrase_train_data = SentencePairDataset(para_train_data, args, isRegression =False)\n",
    "    paraphrase_dev_data = SentencePairDataset(para_dev_data, args, isRegression =False)\n",
    "\n",
    "    paraphrase_dev_dataloader = DataLoader(paraphrase_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=paraphrase_dev_data.collate_fn,  pin_memory=True)\n",
    "\n",
    "    #sts\n",
    "    sts_train_data = SentencePairDataset(sts_train_data, args, isRegression =True)\n",
    "    sts_dev_data = SentencePairDataset(sts_dev_data, args, isRegression =True)\n",
    "\n",
    "    sts_dev_dataloader = DataLoader(sts_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=sts_dev_data.collate_fn,  pin_memory=True)\n",
    "\n",
    "    #MTL data loader\n",
    "    train_datasets = [sst_train_data,paraphrase_train_data, sts_train_data]\n",
    "    #Temporarily initialized here but later in epoch loop to update current epoch and do annealed sampling\n",
    "    mtl_sampler = MultiTaskBatchSampler(        datasets=train_datasets,\n",
    "        current_epoch=1,\n",
    "        total_epochs=args.epochs,\n",
    "        batch_size = args.batch_size,\n",
    "        mix_opt=1,\n",
    "        extra_task_ratio=0,\n",
    "        bin_size=64,\n",
    "        bin_on=False,\n",
    "        bin_grow_ratio=0.5,\n",
    "        sampling='sequential')\n",
    "\n",
    "    multi_task_train_dataset = MultiTaskDataset(train_datasets)\n",
    "\n",
    "    collate_fns = {\n",
    "        0: sst_train_data.collate_fn,\n",
    "        1: paraphrase_train_data.collate_fn,\n",
    "        2: sts_train_data.collate_fn\n",
    "    }\n",
    "\n",
    "    # Creating the custom collate function using the dictionary of collate functions\n",
    "    # Linked to each task id\n",
    "    custom_collate_fn = CustomCollateFn(collate_fns)\n",
    "\n",
    "    multi_task_train_data = DataLoader(\n",
    "    multi_task_train_dataset,\n",
    "    batch_sampler=mtl_sampler,\n",
    "    collate_fn = custom_collate_fn,\n",
    "    pin_memory=True\n",
    "    )\n",
    "else:\n",
    "    sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "    sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train') #Itis correct to use this slit for dev. The other option is test which does not load the labels\n",
    "\n",
    "    #Sentiment analysis\n",
    "    sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "    sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_dev_data.collate_fn)\n",
    "\n",
    "    #Paraphrasing\n",
    "    paraphrase_train_data = SentencePairDataset(para_train_data, args, isRegression =False)\n",
    "    paraphrase_dev_data = SentencePairDataset(para_dev_data, args, isRegression =False)\n",
    "\n",
    "    paraphrase_dev_dataloader = DataLoader(paraphrase_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=paraphrase_dev_data.collate_fn)\n",
    "\n",
    "    #sts\n",
    "    sts_train_data = SentencePairDataset(sts_train_data, args, isRegression =True)\n",
    "    sts_dev_data = SentencePairDataset(sts_dev_data, args, isRegression =True)\n",
    "\n",
    "    sts_dev_dataloader = DataLoader(sts_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=sts_dev_data.collate_fn)\n",
    "\n",
    "    #MTL data loader\n",
    "    train_datasets = [sst_train_data,paraphrase_train_data, sts_train_data]\n",
    "    #Temporarily initialized here but later in epoch loop to update current epoch and do annealed sampling\n",
    "    mtl_sampler = MultiTaskBatchSampler(        datasets=train_datasets,\n",
    "        current_epoch=1,\n",
    "        total_epochs=args.epochs,\n",
    "        batch_size = args.batch_size,\n",
    "        mix_opt=1,\n",
    "        extra_task_ratio=0,\n",
    "        bin_size=64,\n",
    "        bin_on=False,\n",
    "        bin_grow_ratio=0.5,\n",
    "        sampling='sequential')\n",
    "\n",
    "    multi_task_train_dataset = MultiTaskDataset(train_datasets)\n",
    "\n",
    "    collate_fns = {\n",
    "        0: sst_train_data.collate_fn,\n",
    "        1: paraphrase_train_data.collate_fn,\n",
    "        2: sts_train_data.collate_fn\n",
    "    }\n",
    "\n",
    "    # Creating the custom collate function using the dictionary of collate functions\n",
    "    # Linked to each task id\n",
    "    custom_collate_fn = CustomCollateFn(collate_fns)\n",
    "\n",
    "    multi_task_train_data = DataLoader(\n",
    "    multi_task_train_dataset,\n",
    "    batch_sampler=mtl_sampler,\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "    # Init model\n",
    "   \n",
    "# debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_multitask(args)\n",
    "config = {'hidden_dropout_prob': args.hidden_dropout_prob,\n",
    "              'num_labels': 5,\n",
    "              'hidden_size': 768,\n",
    "              'data_dir': '.',\n",
    "              'option': args.option,\n",
    "              'local_files_only': args.local_files_only}\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- initiating the Model  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- initiating the Model  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MultitaskBERT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "best_dev_acc_sst = 0\n",
    "best_dev_acc_paraphrase = 0\n",
    "best_dev_corr_sts = 0\n",
    "\n",
    "best_metric = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-0:   8%|▊         | 399/4878 [02:35<29:04,  2.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iam inside eval\n",
      "iam inside eval 2 \n",
      "cuda\n",
      "Paraphrase detection accuracy: 0.656\n",
      "Sentiment classification accuracy: 0.406\n",
      "Semantic Textual Similarity correlation: 0.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-1:   0%|          | 0/4878 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the model to ./models/\n",
      "model saved\n",
      "Epoch 0: train loss :: 26586.843, dev sentiment acc :: 0.656, dev sentiment acc :: 0.656, dev sts corr :: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-1:   8%|▊         | 399/4878 [02:31<28:16,  2.64it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iam inside eval\n",
      "iam inside eval 2 \n",
      "cuda\n",
      "Paraphrase detection accuracy: 0.656\n",
      "Sentiment classification accuracy: 0.500\n",
      "Semantic Textual Similarity correlation: 0.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-2:   0%|          | 0/4878 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the model to ./models/\n",
      "model saved\n",
      "Epoch 1: train loss :: 28285.774, dev sentiment acc :: 0.656, dev sentiment acc :: 0.656, dev sts corr :: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-2:   8%|▊         | 399/4878 [02:32<28:32,  2.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iam inside eval\n",
      "iam inside eval 2 \n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-3:   0%|          | 0/4878 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase detection accuracy: 0.656\n",
      "Sentiment classification accuracy: 0.438\n",
      "Semantic Textual Similarity correlation: 0.417\n",
      "Epoch 2: train loss :: 46274.549, dev sentiment acc :: 0.656, dev sentiment acc :: 0.656, dev sts corr :: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-3:   8%|▊         | 399/4878 [02:33<28:39,  2.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iam inside eval\n",
      "iam inside eval 2 \n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-4:   0%|          | 0/4878 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase detection accuracy: 0.656\n",
      "Sentiment classification accuracy: 0.438\n",
      "Semantic Textual Similarity correlation: 0.423\n",
      "Epoch 3: train loss :: 18613.340, dev sentiment acc :: 0.656, dev sentiment acc :: 0.656, dev sts corr :: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-4:   8%|▊         | 399/4878 [02:41<30:08,  2.48it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iam inside eval\n",
      "iam inside eval 2 \n",
      "cuda\n",
      "Paraphrase detection accuracy: 0.656\n",
      "Sentiment classification accuracy: 0.406\n",
      "Semantic Textual Similarity correlation: 0.423\n",
      "Epoch 4: train loss :: 19151.710, dev sentiment acc :: 0.656, dev sentiment acc :: 0.656, dev sts corr :: 0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run for the specified number of epochs\n",
    "args.epochs = 5\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "    sst_train_loss_list = []\n",
    "    paraphrase_train_loss_list = []\n",
    "    sts_train_loss_list = []\n",
    "\n",
    "    for batch in tqdm(multi_task_train_data, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "\n",
    "        #Batch loading, prediction and loss depending on task:\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        b_task_id, b_ids, b_mask, b_token_type_ids, b_labels = (\n",
    "        batch['task_id'],\n",
    "        batch['token_ids'].to(device),\n",
    "        batch['attention_mask'].to(device),\n",
    "        batch['token_type_ids'].to(device),\n",
    "        batch['labels'].to(device))\n",
    "        \n",
    "        \n",
    "        logits = model.predict(input_ids=b_ids,attention_mask=b_mask,token_type_ids=b_token_type_ids,task_id=b_task_id)\n",
    "        batch_loss = [0]*3\n",
    "        if b_task_id==0: #Sentiment analysis\n",
    "            sst_loss = F.cross_entropy(logits, b_labels.view(-1), reduction='mean')\n",
    "            batch_loss[b_task_id]=sst_loss\n",
    "            sst_train_loss_list.append(sst_loss.item()) #value, not tensor\n",
    "\n",
    "        elif b_task_id==1: #Paraphrasing\n",
    "            bce_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "            paraphrase_loss=bce_loss(logits.view(-1),b_labels.to(torch.float64)) #Change these logits\n",
    "            batch_loss[b_task_id]=paraphrase_loss\n",
    "            paraphrase_train_loss_list.append(paraphrase_loss.item())\n",
    "\n",
    "        elif b_task_id==2: # Text similarity\n",
    "\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            probabilities = sigmoid(logits) #maps logits to range 0 to 1\n",
    "            # Define the MSE loss function\n",
    "            mse_loss = nn.MSELoss(reduction='mean')\n",
    "            b_labels_scaled = (b_labels / 5).float() #Divide between 5 to match range 0 to 1 of logit. Float required due to loss calculation error\n",
    "            sts_loss = mse_loss(probabilities.view(-1), b_labels_scaled)\n",
    "            batch_loss[b_task_id]=sts_loss\n",
    "            sts_train_loss_list.append(sts_loss.item())\n",
    "                    \n",
    "        else:\n",
    "            raise ValueError(\"Invalid b_task_id value. Expected 0, 1, or 2.\")\n",
    "        \n",
    "\n",
    "        losses_list = [sst_train_loss_list,paraphrase_train_loss_list,sts_train_loss_list]\n",
    "        #Compute weighted loss\n",
    "        weights = compute_loss_weights(losses_list)\n",
    "\n",
    "        total_loss = 0\n",
    "        for loss, weight in zip(batch_loss,weights):\n",
    "            total_loss+=loss*weight\n",
    "            \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += total_loss.item()\n",
    "        num_batches += 1\n",
    "        if num_batches == 400:\n",
    "            break\n",
    "\n",
    "        #End of training batches\n",
    "    \n",
    "    #Start dev evaluation \n",
    "    (dev_paraphrase_accuracy, dev_para_y_pred, dev_para_sent_ids,\n",
    "        dev_sentiment_accuracy,dev_sst_y_pred, dev_sst_sent_ids,\n",
    "        dev_sts_corr, dev_sts_y_pred, dev__sent_ids) = model_eval_multitask(sst_dev_dataloader,\n",
    "                                                                    paraphrase_dev_dataloader,sts_dev_dataloader,model, device  )\n",
    "    \n",
    "    #We have to weight or average the three sores to save the best model.\n",
    "    # In the diven code only sst is used\n",
    "\n",
    "    weighted_avg = 0.333 * dev_sentiment_accuracy + 0.333 * dev_paraphrase_accuracy + 0.333 * ((dev_sts_corr +1) / 2)\n",
    "    \n",
    "    if  weighted_avg >=  best_metric :\n",
    "        best_metric = weighted_avg\n",
    "        save_model(model, optimizer, args, config, args.filepath)\n",
    "        print(\"model saved\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, dev sentiment acc :: {dev_paraphrase_accuracy :.3f}, dev sentiment acc :: {dev_paraphrase_accuracy :.3f}, dev sts corr :: {best_dev_corr_sts :.3f}\")\n",
    "    #Add train metrics to this print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('dnlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd7499b50a9359b0244c088e31359451aafcf905b1d16c7709f0fd43ea3ea1a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
