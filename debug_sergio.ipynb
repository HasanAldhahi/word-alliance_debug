{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is better to copy the code here instead of importing to prevent the arg part from running\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import time, random, numpy as np, argparse\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from types import SimpleNamespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "from datasets import SentenceClassificationDataset, SentencePairDataset, \\\n",
    "    load_multitask_data\n",
    "from bert import BertModel\n",
    "from data_loader import MultiTaskBatchSampler,MultiTaskDataset\n",
    "from optimizer import AdamW\n",
    "\n",
    "from evaluation import model_eval_sst, test_model_multitask, model_eval_multitask, compute_total_loss\n",
    "from tokenizer import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENTIMENT_CLASSES = 5\n",
    "TQDM_DISABLE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "from datasets import DatasetDict #Huggingface library\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, N_SENTIMENT_CLASSES)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', local_files_only=config.local_files_only)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,token_type_ids):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask,token_type_ids) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        sentence_embeddings = dropped.last_hidden_state[:, 0, :]\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def predict(self,input_ids,attention_mask,token_type_ids,task_id):\n",
    "        cls_hidden_state = self.forward(input_ids, attention_mask,token_type_ids)\n",
    "\n",
    "        if task_id==0:\n",
    "            return self.sst_classifier(cls_hidden_state)\n",
    "        elif task_id==1:\n",
    "            return self.para_classifier(cls_hidden_state)\n",
    "        elif task_id==2:\n",
    "            return self.sts_classifier(cls_hidden_state)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task_id value. Expected 0, 1, or 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, args, config, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")\n",
    "\n",
    "#Collate function dependent on current task\n",
    "class CustomCollateFn:\n",
    "    def __init__(self, collate_fns):\n",
    "        self.collate_fns = collate_fns\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        task_id,_= batch[0] #This tuple is defined in the MultiTaskDataset class\n",
    "        #This only works if a batch only contains data from one task\n",
    "        collate_fn = self.collate_fns[task_id]\n",
    "        actual_batch = [actual_batch for _, actual_batch in batch]\n",
    "        return collate_fn(actual_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask(args):\n",
    "    device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "    # Load data\n",
    "    # Create the data and its corresponding datasets and dataloader\n",
    "    sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "    sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train') #Itis correct to use this slit for dev. The other option is test which does not load the labels\n",
    "    \n",
    "    #Sentiment analysis\n",
    "    sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "    sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_dev_data.collate_fn)\n",
    "    \n",
    "    #Paraphrasing\n",
    "    paraphrase_train_data = SentencePairDataset(para_train_data, args, isRegression =False)\n",
    "    paraphrase_dev_data = SentencePairDataset(para_dev_data, args, isRegression =False)\n",
    "\n",
    "    paraphrase_dev_dataloader = DataLoader(paraphrase_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=paraphrase_dev_data.collate_fn)\n",
    "    \n",
    "    #sts\n",
    "    sts_train_data = SentencePairDataset(sts_train_data, args, isRegression =True)\n",
    "    sts_dev_data = SentencePairDataset(sts_dev_data, args, isRegression =True)\n",
    "\n",
    "    sts_dev_dataloader = DataLoader(sts_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=sts_dev_data.collate_fn)\n",
    "    \n",
    "    #MTL data loader\n",
    "    train_datasets = [sst_train_data,paraphrase_train_data, sts_train_data]\n",
    "    #Temporarily initialized here but later in epoch loop to update current epoch and do annealed sampling\n",
    "    mtl_sampler = MultiTaskBatchSampler(        datasets=train_datasets,\n",
    "        current_epoch=1,\n",
    "        total_epochs=5,\n",
    "        batch_size = 128,\n",
    "        mix_opt=1,\n",
    "        extra_task_ratio=0,\n",
    "        bin_size=64,\n",
    "        bin_on=False,\n",
    "        bin_grow_ratio=0.5,\n",
    "        sampling='sequential')\n",
    "\n",
    "    multi_task_train_dataset = MultiTaskDataset(train_datasets)\n",
    "\n",
    "    collate_fns = {\n",
    "        0: sst_train_data.collate_fn,\n",
    "        1: paraphrase_train_data.collate_fn,\n",
    "        2: sts_train_data.collate_fn\n",
    "    }\n",
    "\n",
    "    # Creating the custom collate function using the dictionary of collate functions\n",
    "    # Linked to each task id\n",
    "    custom_collate_fn = CustomCollateFn(collate_fns)\n",
    "\n",
    "    multi_task_train_data = DataLoader(\n",
    "    multi_task_train_dataset,\n",
    "    batch_sampler=mtl_sampler,\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "    # Init model\n",
    "    config = {'hidden_dropout_prob': args.hidden_dropout_prob,\n",
    "              'num_labels': num_labels,\n",
    "              'hidden_size': 768,\n",
    "              'data_dir': '.',\n",
    "              'option': args.option,\n",
    "              'local_files_only': args.local_files_only}\n",
    "\n",
    "    config = SimpleNamespace(**config)\n",
    "\n",
    "    model = MultitaskBERT(config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lr = args.lr\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    best_dev_acc_sst = 0\n",
    "    best_dev_acc_paraphrase = 0\n",
    "    best_dev_corr_sts = 0\n",
    "\n",
    "    # Run for the specified number of epochs\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        sst_train_loss_list = []\n",
    "        paraphrase_train_loss_list = []\n",
    "        sts_train_loss_list = []\n",
    "        #TODO IMPLEMENT A SIMPLE SEQUENTIAL DATALOADER TO TEST\n",
    "        for batch in tqdm(multi_task_train_data, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "\n",
    "            #Batch loading, prediction and loss depending on task:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            b_task_id, b_ids, b_mask,b_token_type_ids, b_labels = (batch['task_id'],batch['token_ids'],\n",
    "                        batch['attention_mask'],batch['token_type_ids'], batch['labels'])\n",
    "\n",
    "            logits = model.predict(input_ids=b_ids,attention_mask=b_mask,token_type_ids=b_token_type_ids,task_id=b_task_id)\n",
    "\n",
    "            if b_task_id==0: #Sentiment analysis\n",
    "\n",
    "                sst_loss = F.cross_entropy(logits, b_labels.view(-1), reduction='mean')\n",
    "                sst_train_loss_list.append(sst_loss)\n",
    "\n",
    "            elif b_task_id==1: #Paraphrasing\n",
    "\n",
    "                bce_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "                paraphrase_loss=bce_loss(logits, b_labels.view(-1)) #Change these logits\n",
    "                paraphrase_train_loss_list.append(paraphrase_loss)\n",
    "\n",
    "            elif b_task_id==2: # Text similarity\n",
    "\n",
    "                sigmoid = nn.Sigmoid()\n",
    "                probabilities = sigmoid(logits) #maps logits to range 0 to 1\n",
    "                # Define the MSE loss function\n",
    "                mse_loss = nn.MSELoss(reduction='mean')\n",
    "                sts_loss = mse_loss(probabilities, b_labels.view(-1))\n",
    "                sts_train_loss_list.append(sts_loss)\n",
    "                        \n",
    "            else:\n",
    "                raise ValueError(\"Invalid b_task_id value. Expected 0, 1, or 2.\")\n",
    "            \n",
    "            losses_list = [sst_train_loss_list,paraphrase_train_loss_list,sts_train_loss_list]\n",
    "            #Compute weighted loss\n",
    "            loss,variances = compute_total_loss(losses_list)\n",
    "            print(\"list of performance variance by training\",variances)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            #End of training batches\n",
    "\n",
    "        #Start dev evaluation \n",
    "        (dev_paraphrase_accuracy, dev_para_y_pred, dev_para_sent_ids,\n",
    "         dev_sentiment_accuracy,dev_sst_y_pred, dev_sst_sent_ids,\n",
    "           dev_sts_corr, dev_sts_y_pred, dev__sent_ids) = model_eval_multitask(sst_dev_dataloader,\n",
    "                                                                      paraphrase_dev_dataloader,sts_dev_dataloader,model, model.device  )\n",
    "        \n",
    "        #We have to weight or average the three sores to save the best model.\n",
    "        # In the diven code only sst is used\n",
    "\n",
    "\n",
    "        if dev_sentiment_accuracy > best_dev_acc_sst and dev_paraphrase_accuracy >best_dev_acc_paraphrase and dev_sts_corr>best_dev_corr_sts:\n",
    "            best_dev_acc_sst = dev_sentiment_accuracy\n",
    "            best_dev_acc_paraphrase = dev_paraphrase_accuracy\n",
    "            best_dev_corr_sts = dev_sts_corr\n",
    "            save_model(model, optimizer, args, config, args.filepath)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, dev sentiment acc :: {dev_paraphrase_accuracy :.3f}, dev sentiment acc :: {dev_paraphrase_accuracy :.3f}, dev sts corr :: {best_dev_corr_sts :.3f}\")\n",
    "        #Add train metrics to this print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tempArgs():\n",
    "    def __init__(self,):\n",
    "        self.local_files_only=True\n",
    "        self.use_gpu = False\n",
    "        self.sst_train=\"data/ids-sst-train.csv\"\n",
    "        self.sst_dev=\"data/ids-sst-dev.csv\"\n",
    "        self.sst_test=\"data/ids-sst-test-student.csv\"\n",
    "        self.para_train=\"data/quora-train.csv\"\n",
    "        self.para_dev=\"data/quora-dev.csv\"\n",
    "        self.para_test=\"data/quora-test-student.csv\"\n",
    "        self.sts_train=\"data/sts-train.csv\"\n",
    "        self.sts_dev=\"data/sts-dev.csv\"\n",
    "        self.sts_test=\"data/sts-test-student.csv\"\n",
    "        self.batch_size = 32\n",
    "        self.hidden_dropout_prob = 0.3\n",
    "        self.option=\"pretrain\"\n",
    "        self.lr = 1e-3\n",
    "        self.epochs = 10\n",
    "tempargs = tempArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n",
      "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
      "Loaded 20212 train examples from data/quora-dev.csv\n",
      "Loaded 863 train examples from data/sts-dev.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultitaskBERT' object has no attribute 'linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_multitask(tempargs)\n",
      "Cell \u001b[1;32mIn[5], line 117\u001b[0m, in \u001b[0;36mtrain_multitask\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39melif\u001b[39;00m b_task_id\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m: \u001b[39m#Paraphrasing\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     b_ids_1, b_mask_1,b_ids_2,b_mask_2, b_labels \u001b[39m=\u001b[39m (batch[\u001b[39m'\u001b[39m\u001b[39mtoken_ids_1\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    114\u001b[0m             batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask_1\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mtoken_ids_2\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    115\u001b[0m             batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask_2\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 117\u001b[0m     logits_paraphrase \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_paraphrase(b_ids_1,\n\u001b[0;32m    118\u001b[0m                 b_mask_1,\n\u001b[0;32m    119\u001b[0m                b_ids_2, \n\u001b[0;32m    120\u001b[0m                b_mask_2)\n\u001b[0;32m    121\u001b[0m     \u001b[39m#TODO. THESE VALUES must be checked\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     bce_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss(reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 61\u001b[0m, in \u001b[0;36mMultitaskBERT.predict_paraphrase\u001b[1;34m(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39mNote that your output should be unnormalized (a logit); it will be passed to the sigmoid function\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mduring evaluation, and handled as a logit by the appropriate loss function.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m### TODO\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m hidden_states_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_ids_1, attention_mask_1)\n\u001b[0;32m     62\u001b[0m hidden_states_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(input_ids_2, attention_mask_2)\n\u001b[0;32m     63\u001b[0m combined_hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden_states_1[:, \u001b[39m0\u001b[39m :], hidden_states_2[:, \u001b[39m0\u001b[39m, :]), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m, in \u001b[0;36mMultitaskBERT.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     32\u001b[0m bert_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(input_ids, attention_mask) \n\u001b[0;32m     33\u001b[0m dropped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(bert_out[\u001b[39m'\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 34\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(dropped)\n\u001b[0;32m     35\u001b[0m sentence_embeddings \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m sentence_embeddings\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:947\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m    946\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m--> 947\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    948\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultitaskBERT' object has no attribute 'linear'"
     ]
    }
   ],
   "source": [
    "train_multitask(tempargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n",
      "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
      "Loaded 20212 train examples from data/quora-dev.csv\n",
      "Loaded 863 train examples from data/sts-dev.csv\n"
     ]
    }
   ],
   "source": [
    "args=tempargs\n",
    "sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train') #Itis correct to use this slit for dev. The other option is test which does not load the labels\n",
    "\n",
    "#Sentiment analysis\n",
    "sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=sst_dev_data.collate_fn)\n",
    "\n",
    "#Paraphrasing\n",
    "paraphrase_train_data = SentencePairDataset(para_train_data, args, isRegression =False)\n",
    "paraphrase_dev_data = SentencePairDataset(para_dev_data, args, isRegression =False)\n",
    "\n",
    "paraphrase_dev_dataloader = DataLoader(paraphrase_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=paraphrase_dev_data.collate_fn)\n",
    "\n",
    "#sts\n",
    "sts_train_data = SentencePairDataset(sts_train_data, args, isRegression =True)\n",
    "sts_dev_data = SentencePairDataset(sts_dev_data, args, isRegression =True)\n",
    "\n",
    "sts_dev_dataloader = DataLoader(sts_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                            collate_fn=sts_dev_data.collate_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_paraphrase = iter(paraphrase_dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_batch= next(batch_paraphrase)\n",
    "list_ids1=current_batch[\"token_ids_1\"].tolist()\n",
    "list_ids2=current_batch[\"token_ids_2\"].tolist()\n",
    "\n",
    "output = paraphrase_train_data.tokenizer.build_inputs_with_special_tokens(token_ids_0=list_ids1,token_ids_1=list_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sst_train_data.dataset\n",
    "\n",
    "sents = [x[0] for x in data[:10]]\n",
    "labels = [x[1] for x in data[:10]]\n",
    "sent_ids = [x[2] for x in data[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_train_data.tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mask = paraphrase_train_data.tokenizer(text=sent1,text_pair=sent2, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mask = sst_train_data.tokenizer(text=sents, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_mask.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] the rock is destined to be the 21st century's new ` ` conan'' and that he's going to make a splash even greater than arnold schwarzenegger, jean - claud van damme or steven segal. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_train_data.tokenizer.decode(batch_mask['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_mask['token_type_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2129,  2079,  1045,  4553,  2151,  2653,  3435,  1029,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  1996,  2190,  3176,  5352,  2005,  6228,  6145,\n",
       "          2005,  2893,  3105,  1029,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  1996,  4489,  2090,  8808, 12731, 17811,  1998,\n",
       "          8808,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2339,  2003,  1996, 11507,  1997,  2495,  2590,  2000,  1037,\n",
       "          3836,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  1996,  2190,  1998, 15282,  3971,  2005,  7494,\n",
       "          2769,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2064,  1045, 17409, 10636,  2007,  6520, 23422,  1029,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2339,  2024,  9130,  1010,  8224,  1010,  1998,  2500,  2025,\n",
       "          3039,  1999,  2859,  1029,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  2070, 16012,  2378, 14192, 17592,  3934,  2008,\n",
       "          2019,  8324,  3076,  2064,  2079,  2007,  7271,  2800,  5906,  1998,\n",
       "         17881,  1029,   102,     0],\n",
       "        [  101,  2054,  2515, 12731, 12171,  1035, 20704, 16558,  2812,  1999,\n",
       "         20868,  6593,  2278,  1029,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1999,  2054,  3971,  2064, 25957,  9953, 11834, 19585,  4048,\n",
       "          2782,  2064,  2022,  6334,  1999,  1037,  2062, 17338,  5379,  2126,\n",
       "          1029,   102,     0,     0],\n",
       "        [  101,  2054,  2024,  1996,  2087, 18988,  1013, 14354,  2389,  2808,\n",
       "          2055,  2166,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  1996,  2190,  3971,  2000,  5335,  2394,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2029,  4684, 13675,  2213,  4007,  2079,  2017,  2228,  2003,\n",
       "          2190,  2005,  2026,  2449,  1029,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  1996,  4489,  2090, 16007,  1998,  4748,  7389,\n",
       "          2080, 14246,  2226,  1999, 11924,  1029,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  1996,  3558,  3574,  1997,  4082, 10004, 25971,\n",
       "          1029,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  1996,  5409,  2518,  2008,  2038,  3047,  2000,\n",
       "          2017,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  1996,  3114,  2369, 18772,  8208,  1997, 16123,\n",
       "         11094,  2854,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2003,  1996,  3915,  1996,  2069,  2406,  2008,  2038, 14658,\n",
       "         14549,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1000,  2129,  2079,  2017, 11178,  2224,  1000,  1000,  2017,\n",
       "          1998,  2033,  2119,  1000,  1000,  1999,  1037,  6251,  1029,  1000,\n",
       "           102,     0,     0,     0],\n",
       "        [  101,  2339,  2024,  2116,  2111, 20505,  1029,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2129,  2079,  1045,  8054,  2026,  3008,  2000,  2292,  2033,\n",
       "          3113,  2019,  3784,  2767,  1029,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2129,  2064,  1045, 12200,  2026,  2394,  3015,  4813,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  3084,  2619,  2031,  1037,  8595,  3085,  2227,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1000,  2054,  2003,  1996,  2190, 12845,  2043,  2619,  2758,\n",
       "          1000,  1000,  2042,  2045,  1010,  2589,  2008,  1000,  1000,  1029,\n",
       "          1000,   102,     0,     0],\n",
       "        [  101,  2003,  8816,  2075,  2694,  2186,  2013,  4840,  8043,  3111,\n",
       "          1012,  5658,  3423,  1998,  3647,  1029,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  2070,  3152,  2008,  2191,  3821,  2298,  1043,\n",
       "         10278, 25373,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2003,  2045,  2151, 12761,  5056,  1997, 13852,  2791,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2064,  1045,  6065,  2871,  2243,  2290,  2044,  1996,  2287,\n",
       "          1997,  2871, 10262,  1045,  2064,  2448,  1015,  3178,  7678,  1022,\n",
       "          2243,  2448,  1029,   102],\n",
       "        [  101,  2129,  2079,  1045,  9462,  3571,  1998,  6245,  1029,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  1996,  3437,  2000,  2023,  3160,  1029,  1006,\n",
       "          2156,  6412,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2515,  7316,  8275,  3415,  2006, 22035,  2527,  2079,  2505,\n",
       "          1029,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2003,  2045,  2151,  3382,  1997,  3101,  2005,  2634,  1999,\n",
       "          5673,  3783,  2355,  1029,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(batch_paraphrase)[\"token_ids_1\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
