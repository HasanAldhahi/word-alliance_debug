{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is better to copy the code here instead of importing to prevent the arg part from running\n",
    "import torch\n",
    "from datasets import SentenceClassificationDataset, SentencePairDataset, \\\n",
    "    load_multitask_data\n",
    "from torch.utils.data import DataLoader\n",
    "from data_loader import MultiTaskBatchSampler,MultiTaskDataset\n",
    "from optimizer import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from types import SimpleNamespace\n",
    "from bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENTIMENT_CLASSES = 5\n",
    "TQDM_DISABLE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', local_files_only=config.local_files_only)\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.sst_classifier = torch.nn.Linear(self.bert.config.hidden_size, N_SENTIMENT_CLASSES)\n",
    "        self.para_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sts_classifier = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        bert_out = self.bert(input_ids, attention_mask) \n",
    "        dropped = self.drop(bert_out['pooler_output'])\n",
    "        out = self.linear(dropped)\n",
    "        sentence_embeddings = out.last_hidden_state[:, 0, :]\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "    def predict_sentiment(self, input_ids, attention_mask):\n",
    "        '''Given a batch of sentences, outputs logits for classifying sentiment.\n",
    "        There are 5 sentiment classes:\n",
    "        (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)\n",
    "        Thus, your output should contain 5 logits for each sentence.\n",
    "        '''\n",
    "        hidden_states = self.forward(input_ids, attention_mask)\n",
    "        # second dropout might be overkill causing over-regularization\n",
    "        # hidden_states = self.drop(hidden_states)\n",
    "        logits = self.sst_classifier(hidden_states[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def predict_paraphrase(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "\n",
    "        ### TODO\n",
    "        hidden_states_1 = self.forward(input_ids_1, attention_mask_1)\n",
    "        hidden_states_2 = self.forward(input_ids_2, attention_mask_2)\n",
    "        combined_hidden_states = torch.cat((hidden_states_1[:, 0 :], hidden_states_2[:, 0, :]), dim=-1)\n",
    "        logits = self.para_classifier(combined_hidden_states)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def predict_similarity(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        hidden_states_1 = self.forward(input_ids_1, attention_mask_1)\n",
    "        hidden_states_2 = self.forward(input_ids_2, attention_mask_2)\n",
    "        # Absolute difference is more efficient and reduces dimensionality. Might not perform as well though.\n",
    "        # torch.abs(hidden_states_1[:, 0, :] - hidden_states_2[:, 0, :])\n",
    "        # logits = self.sts_classifier(combined_rep)\n",
    "        combined_hidden_states = torch.cat((hidden_states_1[:, 0 :], hidden_states_2[:, 0, :]), dim=-1)\n",
    "        logits = self.sts_classifier(combined_hidden_states)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, args, config, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")\n",
    "\n",
    "#Collate function dependent on current task\n",
    "class CustomCollateFn:\n",
    "    def __init__(self, collate_fns):\n",
    "        self.collate_fns = collate_fns\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        task_id,_= batch[0] #This tuple is defined in the MultiTaskDataset class\n",
    "        #This only works if a batch only contains data from one task\n",
    "        collate_fn = self.collate_fns[task_id]\n",
    "        actual_batch = [actual_batch for _, actual_batch in batch]\n",
    "        return collate_fn(actual_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask(args):\n",
    "    device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "    # Load data\n",
    "    # Create the data and its corresponding datasets and dataloader\n",
    "    sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args.sst_train,args.para_train,args.sts_train, split ='train')\n",
    "    sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args.sst_dev,args.para_dev,args.sts_dev, split ='train') #Itis correct to use this slit for dev. The other option is test which does not load the labels\n",
    "    \n",
    "    #Sentiment analysis\n",
    "    sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "    sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=sst_dev_data.collate_fn)\n",
    "    \n",
    "    #Paraphrasing\n",
    "    paraphrase_train_data = SentencePairDataset(para_train_data, args, isRegression =False)\n",
    "    paraphrase_dev_data = SentencePairDataset(para_dev_data, args, isRegression =False)\n",
    "\n",
    "    paraphrase_dev_dataloader = DataLoader(paraphrase_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                    collate_fn=paraphrase_dev_data.collate_fn)\n",
    "    \n",
    "    #sts\n",
    "    sts_train_data = SentencePairDataset(sts_train_data, args, isRegression =True)\n",
    "    sts_dev_data = SentencePairDataset(sts_dev_data, args, isRegression =True)\n",
    "\n",
    "    sts_dev_dataloader = DataLoader(sts_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=sts_dev_data.collate_fn)\n",
    "    \n",
    "    #MTL data loader\n",
    "    train_datasets = [sst_train_data,paraphrase_train_data, sts_train_data]\n",
    "    #Temporarily initialized here but later in epoch loop to update current epoch and do annealed sampling\n",
    "    mtl_sampler = MultiTaskBatchSampler(        datasets=train_datasets,\n",
    "        current_epoch=1,\n",
    "        total_epochs=5,\n",
    "        batch_size = 128,\n",
    "        mix_opt=1,\n",
    "        extra_task_ratio=0,\n",
    "        bin_size=64,\n",
    "        bin_on=False,\n",
    "        bin_grow_ratio=0.5,\n",
    "        sampling='sequential')\n",
    "\n",
    "    multi_task_train_dataset = MultiTaskDataset(train_datasets)\n",
    "\n",
    "    collate_fns = {\n",
    "        0: sst_train_data.collate_fn,\n",
    "        1: paraphrase_train_data.collate_fn,\n",
    "        2: sts_train_data.collate_fn\n",
    "    }\n",
    "\n",
    "    # Creating the custom collate function using the dictionary of collate functions\n",
    "    # Linked to each task id\n",
    "    custom_collate_fn = CustomCollateFn(collate_fns)\n",
    "\n",
    "    multi_task_train_data = DataLoader(\n",
    "    multi_task_train_dataset,\n",
    "    batch_sampler=mtl_sampler,\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "    # Init model\n",
    "    config = {'hidden_dropout_prob': args.hidden_dropout_prob,\n",
    "              'num_labels': num_labels,\n",
    "              'hidden_size': 768,\n",
    "              'data_dir': '.',\n",
    "              'option': args.option,\n",
    "              'local_files_only': args.local_files_only}\n",
    "\n",
    "    config = SimpleNamespace(**config)\n",
    "\n",
    "    model = MultitaskBERT(config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lr = args.lr\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    best_dev_acc_sst = 0\n",
    "    best_dev_acc_paraphrase = 0\n",
    "    best_dev_corr_sts = 0\n",
    "\n",
    "    # Run for the specified number of epochs\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        sst_train_loss_list = []\n",
    "        paraphrase_train_loss_list = []\n",
    "        sts_train_loss_list = []\n",
    "        #TODO IMPLEMENT A SIMPLE SEQUENTIAL DATALOADER TO TEST\n",
    "        for batch in tqdm(multi_task_train_data, desc=f'train-{epoch}', disable=TQDM_DISABLE):\n",
    "            #TODO Here we have to separate tokens according to the task.\n",
    "            #TODO I will have to return the dataset_id in the collate method to \n",
    "            #get it from the same dataloader. Once with the dataset_id I can choose which method and loss to use\n",
    "\n",
    "            b_task_id = batch['task_id']\n",
    "\n",
    "            #Batch loading, prediction and loss depending on task:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if b_task_id==0: #Sentiment analysis\n",
    "                \n",
    "                b_ids, b_mask, b_labels = (batch['token_ids'],\n",
    "                                        batch['attention_mask'], batch['labels'])\n",
    "\n",
    "                b_ids = b_ids.to(device)\n",
    "                b_mask = b_mask.to(device)\n",
    "                b_labels = b_labels.to(device)\n",
    "                logits_sst = model.predict_sentiment(b_ids, b_mask)\n",
    "                sst_loss = F.cross_entropy(logits_sst, b_labels.view(-1), reduction='mean')\n",
    "                sst_train_loss_list.append(sst_loss)\n",
    "\n",
    "            elif b_task_id==1: #Paraphrasing\n",
    "\n",
    "                b_ids_1, b_mask_1,b_ids_2,b_mask_2, b_labels = (batch['token_ids_1'],\n",
    "                        batch['attention_mask_1'], batch['token_ids_2'],\n",
    "                        batch['attention_mask_2'], batch['labels'])\n",
    "                \n",
    "                logits_paraphrase = model.predict_paraphrase(b_ids_1,\n",
    "                            b_mask_1,\n",
    "                           b_ids_2, \n",
    "                           b_mask_2)\n",
    "                #TODO. THESE VALUES must be checked\n",
    "                bce_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "                paraphrase_loss=bce_loss(logits_paraphrase, b_labels.view(-1)) #Change these logits\n",
    "                paraphrase_train_loss_list.append(paraphrase_loss)\n",
    "\n",
    "            elif b_task_id==2: # Text similarity\n",
    "                logits_sts = model.predict_similarity(b_ids, b_mask)\n",
    "                \n",
    "                #Complete paraphrase loss and sts loss. \n",
    "\n",
    "                #TODO. THESE VALUES must be checked\n",
    "                # Apply sigmoid activation to logits\n",
    "                sigmoid = nn.Sigmoid()\n",
    "                probabilities = sigmoid(logits_sts) #maps logits to range 0 to 1\n",
    "                # Define the MSE loss function\n",
    "                mse_loss = nn.MSELoss(reduction='mean')\n",
    "                sts_loss = mse_loss(probabilities, b_labels.view(-1))\n",
    "                sts_train_loss_list.append(sts_loss)\n",
    "                        \n",
    "            else:\n",
    "                raise ValueError(\"Invalid b_task_id value. Expected 0, 1, or 2.\")\n",
    "            \n",
    "            losses_list = [sst_train_loss_list,paraphrase_train_loss_list,sts_train_loss_list]\n",
    "            #Compute weighted loss\n",
    "            loss,variances = compute_total_loss(losses_list)\n",
    "            print(\"list of performance variance by training\",variances)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            #End of training batches\n",
    "\n",
    "        #Start dev evaluation \n",
    "        (dev_paraphrase_accuracy, dev_para_y_pred, dev_para_sent_ids,\n",
    "         dev_sentiment_accuracy,dev_sst_y_pred, dev_sst_sent_ids,\n",
    "           dev_sts_corr, dev_sts_y_pred, dev__sent_ids) = model_eval_multitask(sst_dev_dataloader,\n",
    "                                                                      paraphrase_dev_dataloader,sts_dev_dataloader,model, model.device  )\n",
    "        \n",
    "        #We have to weight or average the three sores to save the best model.\n",
    "        # In the diven code only sst is used\n",
    "\n",
    "\n",
    "        if dev_sentiment_accuracy > best_dev_acc_sst and dev_paraphrase_accuracy >best_dev_acc_paraphrase and dev_sts_corr>best_dev_corr_sts:\n",
    "            best_dev_acc_sst = dev_sentiment_accuracy\n",
    "            best_dev_acc_paraphrase = dev_paraphrase_accuracy\n",
    "            best_dev_corr_sts = dev_sts_corr\n",
    "            save_model(model, optimizer, args, config, args.filepath)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, dev sentiment acc :: {dev_paraphrase_accuracy :.3f}, dev sentiment acc :: {dev_paraphrase_accuracy :.3f}, dev sts corr :: {best_dev_corr_sts :.3f}\")\n",
    "        #Add train metrics to this print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tempArgs():\n",
    "    def __init__(self,):\n",
    "        self.local_files_only=True\n",
    "        self.use_gpu = False\n",
    "        self.sst_train=\"data/ids-sst-train.csv\"\n",
    "        self.sst_dev=\"data/ids-sst-dev.csv\"\n",
    "        self.sst_test=\"data/ids-sst-test-student.csv\"\n",
    "        self.para_train=\"data/quora-train.csv\"\n",
    "        self.para_dev=\"data/quora-dev.csv\"\n",
    "        self.para_test=\"data/quora-test-student.csv\"\n",
    "        self.sts_train=\"data/sts-train.csv\"\n",
    "        self.sts_dev=\"data/sts-dev.csv\"\n",
    "        self.sts_test=\"data/sts-test-student.csv\"\n",
    "        self.batch_size = 32\n",
    "        self.hidden_dropout_prob = 0.3\n",
    "        self.option=\"pretrain\"\n",
    "        self.lr = 1e-3\n",
    "        self.epochs = 10\n",
    "tempargs = tempArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n",
      "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
      "Loaded 20212 train examples from data/quora-dev.csv\n",
      "Loaded 863 train examples from data/sts-dev.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultitaskBERT' object has no attribute 'linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_multitask(tempargs)\n",
      "Cell \u001b[1;32mIn[5], line 117\u001b[0m, in \u001b[0;36mtrain_multitask\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39melif\u001b[39;00m b_task_id\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m: \u001b[39m#Paraphrasing\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     b_ids_1, b_mask_1,b_ids_2,b_mask_2, b_labels \u001b[39m=\u001b[39m (batch[\u001b[39m'\u001b[39m\u001b[39mtoken_ids_1\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    114\u001b[0m             batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask_1\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mtoken_ids_2\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    115\u001b[0m             batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask_2\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 117\u001b[0m     logits_paraphrase \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_paraphrase(b_ids_1,\n\u001b[0;32m    118\u001b[0m                 b_mask_1,\n\u001b[0;32m    119\u001b[0m                b_ids_2, \n\u001b[0;32m    120\u001b[0m                b_mask_2)\n\u001b[0;32m    121\u001b[0m     \u001b[39m#TODO. THESE VALUES must be checked\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     bce_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss(reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 61\u001b[0m, in \u001b[0;36mMultitaskBERT.predict_paraphrase\u001b[1;34m(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39mNote that your output should be unnormalized (a logit); it will be passed to the sigmoid function\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mduring evaluation, and handled as a logit by the appropriate loss function.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m### TODO\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m hidden_states_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_ids_1, attention_mask_1)\n\u001b[0;32m     62\u001b[0m hidden_states_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(input_ids_2, attention_mask_2)\n\u001b[0;32m     63\u001b[0m combined_hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden_states_1[:, \u001b[39m0\u001b[39m :], hidden_states_2[:, \u001b[39m0\u001b[39m, :]), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m, in \u001b[0;36mMultitaskBERT.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     32\u001b[0m bert_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(input_ids, attention_mask) \n\u001b[0;32m     33\u001b[0m dropped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(bert_out[\u001b[39m'\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 34\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(dropped)\n\u001b[0;32m     35\u001b[0m sentence_embeddings \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m sentence_embeddings\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\dnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:947\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m    946\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m--> 947\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    948\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultitaskBERT' object has no attribute 'linear'"
     ]
    }
   ],
   "source": [
    "train_multitask(tempargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
