{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "from datasets import load_multitask_data, SentencePairDataset, SentenceClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train=\"data/ids-sst-train.csv\"\n",
    "sst_dev=\"data/ids-sst-dev.csv\"\n",
    "sst_test=\"data/ids-sst-test-student.csv\"\n",
    "para_train=\"data/quora-train.csv\"\n",
    "para_dev=\"data/quora-dev.csv\"\n",
    "para_test=\"data/quora-test-student.csv\"\n",
    "sts_train=\"data/sts-train.csv\"\n",
    "sts_dev=\"data/sts-dev.csv\"\n",
    "sts_test=\"data/sts-test-student.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tempArgs():\n",
    "    def __init__(self,):\n",
    "        self.local_files_only=True\n",
    "tempargs = tempArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n",
      "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
      "Loaded 20212 train examples from data/quora-dev.csv\n",
      "Loaded 863 train examples from data/sts-dev.csv\n"
     ]
    }
   ],
   "source": [
    "#Source data. Still need to turn into dataset\n",
    "sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data( sst_train, para_train, sts_train, split ='train')\n",
    "sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data( sst_dev, para_dev, sts_dev, split ='train')\n",
    "#Data source to dataset\n",
    "sst_train_data = SentenceClassificationDataset(sst_train_data, tempargs)\n",
    "sst_dev_data = SentenceClassificationDataset(sst_dev_data, tempargs)\n",
    "\n",
    "paraphrase_train_data = SentencePairDataset(para_train_data, tempargs, isRegression =False)\n",
    "paraphrase_dev_data = SentencePairDataset(para_dev_data, tempargs, isRegression =False)\n",
    "\n",
    "sts_train_data = SentencePairDataset(sts_train_data, tempargs, isRegression =True)\n",
    "sts_dev_data = SentencePairDataset(sts_dev_data, tempargs, isRegression =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [sst_train_data,paraphrase_train_data, sts_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import MultiTaskBatchSampler, MultiTaskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [sst_train_data,paraphrase_train_data, sts_train_data]\n",
    "mtl_sampler = MultiTaskBatchSampler(        datasets=train_datasets,\n",
    "        current_epoch=1,\n",
    "        total_epochs=5,\n",
    "        batch_size = 128,\n",
    "        mix_opt=1,\n",
    "        extra_task_ratio=0,\n",
    "        bin_size=64,\n",
    "        bin_on=False,\n",
    "        bin_grow_ratio=0.5,\n",
    "        sampling='sequential')\n",
    "\n",
    "multi_task_train_dataset = MultiTaskDataset(train_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollateFn:\n",
    "    def __init__(self, collate_fns):\n",
    "        self.collate_fns = collate_fns\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        task_id,_= batch[0] #This tuple is defined in the MultiTaskDataset class\n",
    "        #This only works if a batch only contains data from one task\n",
    "        collate_fn = self.collate_fns[task_id]\n",
    "        actual_batch = [actual_batch for task_id, actual_batch in batch]\n",
    "        return collate_fn(actual_batch)\n",
    "\n",
    "# Assuming you have collate functions for each task\n",
    "collate_fns = {\n",
    "    0: sst_train_data.collate_fn,\n",
    "    1: paraphrase_train_data.collate_fn,\n",
    "    2: sts_train_data.collate_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the custom collate function using the dictionary of collate functions\n",
    "custom_collate_fn = CustomCollateFn(collate_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "multi_task_train_data = DataLoader(\n",
    "    multi_task_train_dataset,\n",
    "    batch_sampler=mtl_sampler,\n",
    "    collate_fn = custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids_1': tensor([[ 101, 2129, 2097,  ...,    0,    0,    0],\n",
      "        [ 101, 2029, 3152,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 6475, 1998,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 3633,  ...,    0,    0,    0]]), 'token_type_ids_1': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask_1': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_ids_2': tensor([[ 101, 2129, 2097,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2065,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 3633,  ...,    0,    0,    0]]), 'token_type_ids_2': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask_2': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0]), 'sent_ids': ['9d7c4fbbfe8708af31741a956', 'e6e24bc9593d711beffc1f9c0', '7e4a530c684917aaa528d021d', '4c578ccc66ca4cd8ee2ed7699', 'c54093cfa53c7112f4602a62e', '3e62c23e5d703e930c4d6ed09', '9e857a932a910fab68e65a5a1', '03e9265e31399ba85c6a418af', 'd6e021efa3c90cdf8df81f3fb', 'a5c4fcab1d8bd690c8a0df360', '21199e2e60f1a0e4f5fd0ac9d', 'c492e72e96b002266ef20de2e', '18c68981a1c541538ddd43bd7', '237ccbe883f2e492b5907b76e', 'c8af7f91d3aeb1e722434077b', 'a30279cb09b2c9d11b7cd7a91', '3e9bae385c5a703e016e99dcf', '223d1eb90198d73e92b57f3cb', 'e35d4f44a6b4f41e9cd60215e', 'c8d8be3509f1fed471cdd3595', 'b6f051fbda7569f0b9afbd85f', 'fd8d00235e7f5e036f6006132', 'bd91d916efeb389867174f26b', '6e8a11dce99c2dd11d5c73982', '635e5d25646dfeee0ea29572e', '0ee380974a317c6e24423d97a', 'ace893d19487aa5856052869c', '1191dd29ce546388fab253f4a', '4538060d9ee77ab491f40b72c', '943e27e45af972c2b0495cfe2', '1c6e25b2fd4cc5e19493b33e3', 'b1c0c03a139bd705d6033b1dc', 'd50a72a11ee8a3e6cfa243d98', '14c525108baf934ffe8f7f4fa', '6ac11e1b61f956190be37f2a4', 'cb4040f52f966e4e146f6324e', '42a996e6116f349be6a426641', 'd47fcd27373e869521ae02ee3', '5330b0fc4e76b980a78dbaece', 'fed8ff65e55bca2facbd1f51f', 'a96946cfec00e56b364b56bd6', 'd61a63e41b6a42785a13801dd', 'f4d40e8359fa8e169958bf455', '58997ebfbf40618d40dc5c5eb', 'fa3382bff5a7b40b5aaff8c83', '95a99d76ac9fe70462bea6a50', 'd34afd7b42562ac93914d8e89', '17aac4af9ff8fe2f221e755a0', 'dd004717a99858e7561246ade', '49eab6f84edb8ea2107bd8b34', '71fd29e0441a083a2aa0f248e', '824f7f0b02544a2824f1be85e', '2de119f60ddd159ecab0a4f1c', 'a5fa826eaa3bcb709c1a42896', '5dd74cb92ed9eeeca9f309b86', '73d35f0245da3d6d999121139', '8167ad301a8d8f67c2ecd470b', '0acf5bfc83d63e29fd5b88ef3', 'ed082e86ee001a0a4ef5426e7', 'f4df233609a114dada0f8656b', '85e1d1c90a3c516127bc30e5a', 'f7c98d4decee7059b9c59ef81', '2f6eddb864ec3e4c154c7cc9f', 'dea867d0def256228ed8c6f2a', '51fa4176a6ace9092d1882181', 'cc55c3dbee931790165c7cc62', '554b6a8253adce33318a8e1ee', 'a5400208d62c6b0d533412ad3', 'cff35a735a3dc3e1ab4760535', 'de4178bd26920319673258c1c', 'b6c00a7174b7a165a5c677951', '8a177728438aceee586a54d7d', 'f106db324ad03e5ce73c79ffe', '74fd3526c72af40a412d76468', '3278cd66f3b586eeef5de306e', 'e7d3e61cb76f9cfcd3782968a', 'efd0e0243dd8a435d185c25c0', '2dc7d667284f590cfe64bf8b0', 'cf4b31354f4f2329aaefa85ec', '12357fbfba36285737f41ca83', '4444a1578410607c44c7c8f45', '739b9cc42f0f37c52ed5e5c8e', '6dbfe5b47349a897a1eeca45d', 'd093805e97bd030a017af1763', '2498c5e379e4b019c7687ef2e', '53ddcb1df06d38b43ba816380', '44b3981100ae0da2426d4135d', '3bdca247a9c47ad3115bc7e64', '0239636520c7ec9b2af2c15ff', '7ff28a5657668689f4a61060d', '5be342e5d2c7ae7756c0880e8', '539d81cb0b99c5484ea22ad0b', '462e29656922da48c99da8d54', 'f86075d023c38db13f4b48188', '6ca86247fa37f73b513e9c460', '96d6917c7b9b303b95767f01a', '0ab136ae591f45583ec5874a0', '996d592b01192e17cecc86c51', '3fea4243348185e593d1daab8', 'b1112643df343c7af31a04291', '5082f6e4e4d9652e1fcf5d774', 'c8848bcfd44e6c2aaae45bfc7', '78443cb83b0d9d04da2203da4', '54d6e3da013ae6ab575b8f7ab', '1ec4f1b4f79be3184d23045ed', '2522fcfc6cb8859aefa7c836c', '577e294ed57b044887ce074d4', 'b65bf7232d3e4215bb0c78a27', 'e47e11c747af7582c38122ae4', '468320952595b193241410320', 'b3f53f914830caa627e78d60a', '5b9cdb1f17e682dfc8dd8ac9e', 'ba749929e4c406590272a040d', '2773cfdd82167679de3cece4d', '00e767b8e07260f945eab134c', '5004b87acbddbb3c4ef391ec4', '1bebbfebeba190d1f3ed95f56', '585c2c7f81c74565917c15444', '827bb08856d8cdd47ee279882', '8edd9eb1eaa7302788c65a5dc', 'b121918aa6404d7240dd20deb', 'b104c867a2269d28641bc66cc', '7bac18801b79035bc87c34c68', '943bc1a80ce2c623add460762', '204e2729bc54dfd93dadcb961', '4ea7b6593ee7a1325cb911a80', 'd9e106468d0f64238f4c31717', 'b222e64f80ff283cf31d2ddf8']}\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for batch in multi_task_train_data:\n",
    "    print(batch)\n",
    "    i=i+1\n",
    "    if i==1:\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
